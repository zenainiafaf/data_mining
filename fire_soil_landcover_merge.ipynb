{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b35942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FUSION SPATIALE : FIRE+SOIL+LANDCOVER (DISTANCE MINIMALE POUR POINTS ORPHELINS)\n",
      "================================================================================\n",
      "\n",
      "[1/8] Chargement des donn√©es...\n",
      "   üìç Chargement du dataset FIRE+SOIL...\n",
      "      ‚úì Fire+Soil charg√© : 140,165 points\n",
      "\n",
      "   üå≥ Chargement du dataset LANDCOVER...\n",
      "      ‚úì Landcover charg√© : 438,513 polygones\n",
      "\n",
      "[2/8] Conversion en GeoDataFrames...\n",
      "   üìç Cr√©ation des points FIRE+SOIL...\n",
      "      ‚úì 140,165 points cr√©√©s\n",
      "   üå≥ Conversion des polygones LANDCOVER...\n",
      "      ‚úì 438,513 polygones cr√©√©s\n",
      "\n",
      "[3/8] Jointure spatiale (intersects) pour points dans polygones...\n",
      "      ‚úì Points match√©s : 139,479 correspondances\n",
      "\n",
      "[4/8] Gestion des doublons...\n",
      "   ‚úÖ Aucun doublon d√©tect√©\n",
      "\n",
      "[5/8] Identification des points orphelins...\n",
      "   üìä Points sans polygone : 686 (0.49%)\n",
      "   üîß Attribution des points orphelins via DISTANCE MINIMALE\n",
      "\n",
      "[6/8] Attribution des points orphelins au polygone le plus proche...\n",
      "      ‚úì 686 points orphelins enrichis\n",
      "\n",
      "[7/8] Fusion des points match√©s et orphelins...\n",
      "      ‚úì Dataset final : 140,165 points\n",
      "\n",
      "[8/8] R√©organisation des colonnes et sauvegarde...\n",
      "\n",
      "‚úÖ FUSION FIRE+SOIL+LANDCOVER TERMIN√âE AVEC SUCC√àS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"FUSION SPATIALE : FIRE+SOIL+LANDCOVER (DISTANCE MINIMALE POUR POINTS ORPHELINS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Chemins des fichiers\n",
    "base_path = r\"C:\\Users\\pc\\Desktop\\DM\\datasets\"\n",
    "fire_soil_path = os.path.join(base_path, \"merged_fire_soil\", \"fire_soil_merged_knn_idw.csv\")\n",
    "landcover_path = os.path.join(base_path, \"landcover\", \"landcover_merged_encoded.csv\")\n",
    "output_dir = os.path.join(base_path, \"merged_fire_soil_landcover_distance_min\")\n",
    "\n",
    "# Cr√©er le dossier de sortie\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 1 : CHARGER LES DONN√âES\n",
    "# ============================================================================\n",
    "print(\"\\n[1/8] Chargement des donn√©es...\")\n",
    "\n",
    "print(\"   üìç Chargement du dataset FIRE+SOIL...\")\n",
    "df_fire_soil = pd.read_csv(fire_soil_path)\n",
    "print(f\"      ‚úì Fire+Soil charg√© : {len(df_fire_soil):,} points\")\n",
    "\n",
    "print(\"\\n   üå≥ Chargement du dataset LANDCOVER...\")\n",
    "df_landcover = pd.read_csv(landcover_path)\n",
    "print(f\"      ‚úì Landcover charg√© : {len(df_landcover):,} polygones\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 2 : CONVERTIR EN GEODATAFRAMES\n",
    "# ============================================================================\n",
    "print(\"\\n[2/8] Conversion en GeoDataFrames...\")\n",
    "\n",
    "# Fire+Soil ‚Üí Points\n",
    "print(\"   üìç Cr√©ation des points FIRE+SOIL...\")\n",
    "geometry_fire_soil = [Point(xy) for xy in zip(df_fire_soil['longitude'], df_fire_soil['latitude'])]\n",
    "gdf_fire_soil = gpd.GeoDataFrame(df_fire_soil, geometry=geometry_fire_soil, crs='EPSG:4326')\n",
    "gdf_fire_soil['point_index'] = gdf_fire_soil.index  # Garder l'index original\n",
    "print(f\"      ‚úì {len(gdf_fire_soil):,} points cr√©√©s\")\n",
    "\n",
    "# Landcover ‚Üí Polygones\n",
    "print(\"   üå≥ Conversion des polygones LANDCOVER...\")\n",
    "df_landcover['geometry'] = df_landcover['geometry'].apply(wkt.loads)\n",
    "gdf_landcover = gpd.GeoDataFrame(df_landcover, geometry='geometry', crs='EPSG:4326')\n",
    "print(f\"      ‚úì {len(gdf_landcover):,} polygones cr√©√©s\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 3 : JOINTURE SPATIALE (POINTS DANS POLYGONES)\n",
    "# ============================================================================\n",
    "print(\"\\n[3/8] Jointure spatiale (intersects) pour points dans polygones...\")\n",
    "\n",
    "gdf_matched = gpd.sjoin(\n",
    "    gdf_fire_soil,\n",
    "    gdf_landcover,\n",
    "    how='inner',\n",
    "    predicate='intersects'\n",
    ")\n",
    "\n",
    "print(f\"      ‚úì Points match√©s : {len(gdf_matched):,} correspondances\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 4 : GESTION DES DOUBLONS\n",
    "# ============================================================================\n",
    "print(\"\\n[4/8] Gestion des doublons...\")\n",
    "\n",
    "duplicated = gdf_matched[gdf_matched.duplicated(subset=['point_index'], keep=False)]\n",
    "if len(duplicated) > 0:\n",
    "    n_unique_dups = duplicated['point_index'].nunique()\n",
    "    print(f\"   ‚ö†Ô∏è  {n_unique_dups:,} points dans plusieurs polygones\")\n",
    "    print(f\"   üîß Application : keep='first'\")\n",
    "    gdf_matched = gdf_matched.drop_duplicates(subset=['point_index'], keep='first')\n",
    "    print(f\"      ‚úì {len(gdf_matched):,} points uniques apr√®s d√©doublonnage\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Aucun doublon d√©tect√©\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 5 : IDENTIFIER LES POINTS ORPHELINS (SANS POLYGONE)\n",
    "# ============================================================================\n",
    "print(\"\\n[5/8] Identification des points orphelins...\")\n",
    "\n",
    "matched_indices = set(gdf_matched['point_index'].values)\n",
    "orphan_mask = ~gdf_fire_soil['point_index'].isin(matched_indices)\n",
    "gdf_orphans = gdf_fire_soil[orphan_mask].copy()\n",
    "n_orphans = len(gdf_orphans)\n",
    "print(f\"   üìä Points sans polygone : {n_orphans:,} ({n_orphans/len(gdf_fire_soil)*100:.2f}%)\")\n",
    "\n",
    "if n_orphans == 0:\n",
    "    print(f\"   ‚úÖ Tous les points sont dans des polygones !\")\n",
    "    gdf_final = gdf_matched.copy()\n",
    "else:\n",
    "    print(f\"   üîß Attribution des points orphelins via DISTANCE MINIMALE\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 6 : ATTRIBUTION DES POINTS ORPHELINS AU POLYGONE LE PLUS PROCHE\n",
    "# ============================================================================\n",
    "print(\"\\n[6/8] Attribution des points orphelins au polygone le plus proche...\")\n",
    "\n",
    "gdf_orphans_enriched = gdf_orphans.copy()\n",
    "landcover_cols_to_copy = [col for col in gdf_landcover.columns if col != 'geometry']\n",
    "\n",
    "for idx, point in gdf_orphans_enriched.iterrows():\n",
    "    distances = gdf_landcover.geometry.distance(point.geometry)\n",
    "    closest_idx = distances.idxmin()\n",
    "    # Copier les colonnes LANDCOVER\n",
    "    for col in landcover_cols_to_copy:\n",
    "        gdf_orphans_enriched.at[idx, col] = gdf_landcover.at[closest_idx, col]\n",
    "\n",
    "print(f\"      ‚úì {len(gdf_orphans_enriched):,} points orphelins enrichis\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 7 : FUSIONNER TOUS LES POINTS\n",
    "# ============================================================================\n",
    "print(\"\\n[7/8] Fusion des points match√©s et orphelins...\")\n",
    "\n",
    "common_cols = list(set(gdf_matched.columns) & set(gdf_orphans_enriched.columns))\n",
    "gdf_final = pd.concat([\n",
    "    gdf_matched[common_cols],\n",
    "    gdf_orphans_enriched[common_cols]\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"      ‚úì Dataset final : {len(gdf_final):,} points\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 8 : R√âORGANISATION ET SAUVEGARDE\n",
    "# ============================================================================\n",
    "print(\"\\n[8/8] R√©organisation des colonnes et sauvegarde...\")\n",
    "\n",
    "cols_to_drop = ['index_right', 'point_index']  # suppression des colonnes techniques\n",
    "gdf_final = gdf_final.drop(columns=[col for col in cols_to_drop if col in gdf_final.columns])\n",
    "\n",
    "# R√©organiser colonnes\n",
    "all_columns = list(gdf_final.columns)\n",
    "base_cols = ['latitude', 'longitude']\n",
    "target_col = ['class']\n",
    "geo_col = ['geometry']\n",
    "feature_cols = [col for col in all_columns if col not in base_cols + target_col + geo_col]\n",
    "new_order = base_cols + feature_cols + target_col + geo_col\n",
    "gdf_final = gdf_final[new_order]\n",
    "\n",
    "# Conversion en GeoDataFrame\n",
    "gdf_final = gpd.GeoDataFrame(gdf_final, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Sauvegarde\n",
    "csv_output = os.path.join(output_dir, \"fire_soil_landcover_merged_distance_min.csv\")\n",
    "df_csv = pd.DataFrame(gdf_final.drop(columns=['geometry']))\n",
    "df_csv.to_csv(csv_output, index=False, encoding='utf-8-sig')\n",
    "\n",
    "gpkg_output = os.path.join(output_dir, \"fire_soil_landcover_merged_distance_min.gpkg\")\n",
    "gdf_final.to_file(gpkg_output, driver='GPKG')\n",
    "\n",
    "try:\n",
    "    excel_output = os.path.join(output_dir, \"fire_soil_landcover_merged_distance_min.xlsx\")\n",
    "    df_csv.to_excel(excel_output, index=False, engine='openpyxl')\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Excel non cr√©√© : {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ FUSION FIRE+SOIL+LANDCOVER TERMIN√âE AVEC SUCC√àS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dceb5bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUPPRESSION DES COLONNES DE M√âTADONN√âES\n",
      "================================================================================\n",
      "\n",
      "[1/3] Chargement du fichier...\n",
      "   ‚úì Fichier charg√© : 140,165 lignes √ó 16 colonnes\n",
      "\n",
      "[2/3] Suppression des colonnes de m√©tadonn√©es...\n",
      "\n",
      "   üìã Colonnes √† supprimer :\n",
      "      ‚Ä¢ knn_distance_km\n",
      "      ‚Ä¢ knn_distance\n",
      "      ‚Ä¢ assigned_method\n",
      "\n",
      "   ‚úì 3 colonnes supprim√©es\n",
      "\n",
      "   üìä Nouveau dataset : 140,165 lignes √ó 13 colonnes\n",
      "\n",
      "[3/3] Sauvegarde du fichier nettoy√©...\n",
      "   ‚úì CSV sauvegard√© : C:\\Users\\pc\\Desktop\\DM\\datasets\\merged_fire_soil_landcover_distance_min\\fire_soil_landcover_merged_clean.csv\n",
      "   ‚úì Excel sauvegard√© : C:\\Users\\pc\\Desktop\\DM\\datasets\\merged_fire_soil_landcover_distance_min\\fire_soil_landcover_merged_clean.xlsx\n",
      "\n",
      "================================================================================\n",
      "‚úÖ NETTOYAGE TERMIN√â\n",
      "================================================================================\n",
      "\n",
      "üìä R√âSUM√â :\n",
      "   ‚Ä¢ Colonnes supprim√©es : 3\n",
      "   ‚Ä¢ Colonnes restantes  : 13\n",
      "   ‚Ä¢ Lignes conserv√©es   : 140,165\n",
      "\n",
      "üìã COLONNES FINALES (13) :\n",
      "    1. latitude\n",
      "    2. longitude\n",
      "    3. TEB_log\n",
      "    4. REF_BULK_log\n",
      "    5. CEC_CLAY\n",
      "    6. GYPSUM_log\n",
      "    7. TEXTURE_SOTER_encoded\n",
      "    8. ORG_CARBON_log\n",
      "    9. LCCCODE_encoded\n",
      "   10. ELEC_COND_log\n",
      "   11. COARSE\n",
      "   12. TEXTURE_USDA_encoded\n",
      "   13. class\n",
      "\n",
      "üëÄ APER√áU DES DONN√âES (5 premi√®res lignes) :\n",
      "================================================================================\n",
      "   latitude  longitude   TEB_log  REF_BULK_log  CEC_CLAY\n",
      "0  36.74886    6.25409  2.995732      0.970779      53.0\n",
      "1  35.87978    4.44782  3.806662      1.040277      48.0\n",
      "2  35.70751    5.53337  3.806662      1.040277      48.0\n",
      "3  32.27667    3.98647  3.806662      0.996949      71.0\n",
      "4  32.40079    4.00642  3.806662      0.996949      71.0\n",
      "\n",
      "================================================================================\n",
      "üöÄ FICHIER NETTOY√â PR√äT √Ä UTILISER !\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# SUPPRESSION DES COLONNES DE M√âTADONN√âES\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"SUPPRESSION DES COLONNES DE M√âTADONN√âES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Chemin du fichier\n",
    "base_path = r\"C:\\Users\\pc\\Desktop\\DM\\datasets\"\n",
    "input_file = os.path.join(base_path, \"merged_fire_soil_landcover_distance_min\", \"fire_soil_landcover_merged_distance_min.csv\")\n",
    "output_file = os.path.join(base_path, \"merged_fire_soil_landcover_distance_min\", \"fire_soil_landcover_merged_clean.csv\")\n",
    "\n",
    "# Colonnes √† supprimer\n",
    "cols_to_drop = ['knn_distance_km', 'knn_distance', 'assigned_method']\n",
    "\n",
    "# ============================================================================\n",
    "# CHARGEMENT ET TRAITEMENT\n",
    "# ============================================================================\n",
    "print(\"\\n[1/3] Chargement du fichier...\")\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"   ‚úì Fichier charg√© : {len(df):,} lignes √ó {len(df.columns)} colonnes\")\n",
    "\n",
    "print(\"\\n[2/3] Suppression des colonnes de m√©tadonn√©es...\")\n",
    "\n",
    "# V√©rifier quelles colonnes existent\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "missing_cols = [col for col in cols_to_drop if col not in df.columns]\n",
    "\n",
    "if len(existing_cols_to_drop) > 0:\n",
    "    print(f\"\\n   üìã Colonnes √† supprimer :\")\n",
    "    for col in existing_cols_to_drop:\n",
    "        print(f\"      ‚Ä¢ {col}\")\n",
    "    \n",
    "    # Supprimer les colonnes\n",
    "    df = df.drop(columns=existing_cols_to_drop)\n",
    "    \n",
    "    print(f\"\\n   ‚úì {len(existing_cols_to_drop)} colonnes supprim√©es\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è  Aucune des colonnes sp√©cifi√©es n'existe dans le fichier\")\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Colonnes non trouv√©es (d√©j√† absentes) :\")\n",
    "    for col in missing_cols:\n",
    "        print(f\"      ‚Ä¢ {col}\")\n",
    "\n",
    "print(f\"\\n   üìä Nouveau dataset : {len(df):,} lignes √ó {len(df.columns)} colonnes\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAUVEGARDE\n",
    "# ============================================================================\n",
    "print(\"\\n[3/3] Sauvegarde du fichier nettoy√©...\")\n",
    "\n",
    "# Sauvegarder en CSV\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"   ‚úì CSV sauvegard√© : {output_file}\")\n",
    "\n",
    "# Optionnel : Sauvegarder en Excel\n",
    "try:\n",
    "    excel_output = output_file.replace('.csv', '.xlsx')\n",
    "    df.to_excel(excel_output, index=False, engine='openpyxl')\n",
    "    print(f\"   ‚úì Excel sauvegard√© : {excel_output}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Excel non cr√©√© : {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RAPPORT FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ NETTOYAGE TERMIN√â\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä R√âSUM√â :\")\n",
    "print(f\"   ‚Ä¢ Colonnes supprim√©es : {len(existing_cols_to_drop)}\")\n",
    "print(f\"   ‚Ä¢ Colonnes restantes  : {len(df.columns)}\")\n",
    "print(f\"   ‚Ä¢ Lignes conserv√©es   : {len(df):,}\")\n",
    "\n",
    "print(f\"\\nüìã COLONNES FINALES ({len(df.columns)}) :\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nüëÄ APER√áU DES DONN√âES (5 premi√®res lignes) :\")\n",
    "print(\"=\" * 80)\n",
    "display_cols = df.columns[:5].tolist()  # Afficher les 5 premi√®res colonnes\n",
    "print(df[display_cols].head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöÄ FICHIER NETTOY√â PR√äT √Ä UTILISER !\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
