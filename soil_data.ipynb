{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3757cf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTRACTION DONN√âES DE SOL HWSD2 - FORMAT POINTS (LAT/LON)\n",
      "================================================================================\n",
      "\n",
      "[1/6] Chargement du shapefile...\n",
      "   ‚úì Shapefile charg√© : 2 entit√©s\n",
      "   ‚úì CRS : GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n",
      "\n",
      "[2/6] Extraction des donn√©es de HWSD2.mdb (LAYER = D1)...\n",
      "   ‚úì Connexion √† la base de donn√©es √©tablie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_12756\\3407815944.py:89: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_sol_D1 = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Donn√©es extraites : 58405 enregistrements (couche D1)\n",
      "   ‚úì Features extraites : 22 attributs\n",
      "\n",
      "[3/6] Clippage du raster HWSD2.bil...\n",
      "   ‚úì Raster ouvert : 43200 x 21600 pixels\n",
      "   ‚úì CRS raster : OGC:CRS84\n",
      "   ‚ö† Reprojection du shapefile vers OGC:CRS84\n",
      "   ‚úì Raster clipp√© sauvegard√©\n",
      "\n",
      "[4/6] Extraction des points (longitude, latitude) du raster...\n",
      "   üìç Filtrage : latitude > 32.0¬∞N (zone Nord)\n",
      "   üéØ √âchantillonnage : 1 point tous les 10 pixels\n",
      "   ‚è≥ Extraction de 3,295,088 pixels valides...\n",
      "   ‚úì Pixels totaux           : 3,295,088\n",
      "   ‚úì Apr√®s √©chantillonnage   : 329,509\n",
      "   ‚úì Points Nord (lat>32.0¬∞) : 83,845\n",
      "   ‚úì Taux de r√©duction       : 97.5%\n",
      "\n",
      "[5/6] √âchantillonnage al√©atoire final...\n",
      "   üìä Points avant √©chantillonnage final : 83,845\n",
      "   üé≤ √âchantillonnage al√©atoire vers 25,000 points...\n",
      "   ‚úì √âchantillonnage termin√© : 25,000 points\n",
      "   ‚úì Types de sol uniques    : 333\n",
      "   ‚úì R√©duction totale        : 99.24%\n",
      "   ‚úì Dataset final : 25,000 points\n",
      "\n",
      "[6/6] Jointure avec les attributs D1...\n",
      "   ‚ö† Lignes apr√®s jointure   : 58,710\n",
      "   üîß D√©doublonnage effectu√© : 58,710 ‚Üí 25,000 lignes\n",
      "   ‚úì 33,710 doublons supprim√©s\n",
      "   ‚úì Jointure effectu√©e : 25,000 points uniques\n",
      "   ‚úì Colonnes finales : 24 (longitude, latitude + 22 attributs)\n",
      "\n",
      "[Sauvegarde] Exportation des r√©sultats...\n",
      "   ‚úì CSV sauvegard√© : C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\HWSD2_D1_points.csv\n",
      "   ‚úì Taille du fichier : 25,000 lignes √ó 24 colonnes\n",
      "\n",
      "================================================================================\n",
      "R√âSUM√â DES DONN√âES (Couche D1 - 0-20cm de profondeur)\n",
      "================================================================================\n",
      "\n",
      "üìç Zone d'√©tude : 25,000 points\n",
      "üìä Features extraites : 22 attributs\n",
      "\n",
      "üìã Aper√ßu des premi√®res lignes :\n",
      " longitude  latitude  COARSE  SAND  SILT  CLAY  TEXTURE_USDA TEXTURE_SOTER  BULK  REF_BULK  ORG_CARBON  PH_WATER  TOTAL_N  CN_RATIO  CEC_SOIL  CEC_CLAY  CEC_EFF   TEB  BSAT  ALUM_SAT  ESP  TCARBON_EQ    GYPSUM  ELEC_COND\n",
      "  7.854167 35.354167       9    55    30    15          11.0             M  1.42      1.62       0.589       8.2     0.78       9.0        14        83     37.0  37.0    99         0    4         9.3  3.300000          1\n",
      "  6.004167 35.262500       4    43    37    20           9.0             M  1.46      1.71       0.727       8.2     1.01       9.0        16        72     36.0  36.0   100         0    3         8.4  2.200000          1\n",
      " -0.995833 33.137500      18    66    22    12          11.0             C  1.37      1.56       0.711       7.4     0.58      13.0         6        35     25.0  10.0    64         0    3         6.3  0.300000          1\n",
      "  8.712500 34.245833       2    53    32    15          11.0             M  1.19      1.62       0.429       8.1     0.65       8.0         7        37    119.0 119.0   100         0    2        15.2 57.599998          2\n",
      "  0.237500 35.954167       5    33    31    36           5.0             F  1.35      1.90       1.930       6.1     1.32      12.0        20        34     15.0  17.0    82         0    1         0.0  1.600000          1\n",
      "  7.387500 35.545833      12    49    33    18           9.0             M  1.45      1.68       0.697       8.2     0.80       9.0        14        67     38.0  38.0   100         0    2        13.9  0.200000          1\n",
      "  5.045833 32.220833      12    49    33    18           9.0             M  1.45      1.68       0.697       8.2     0.80       9.0        14        67     38.0  38.0   100         0    2        13.9  0.200000          1\n",
      " -0.562500 34.312500       3    40    40    20           9.0             M  1.49      1.71       0.695       8.2     0.92       9.0        16        71     37.0  38.0    97         0    7        11.3  0.600000          2\n",
      "  7.962500 32.204167      -4    -4    -4    -4           NaN             - -4.00       NaN      -4.000      -4.0    -4.00      -4.0        -4        -4     -4.0  -4.0    -4        -4   -4        -4.0 -4.000000         -4\n",
      "  8.420833 35.262500       4    43    37    20           9.0             M  1.46      1.71       0.727       8.2     1.01       9.0        16        72     36.0  36.0   100         0    3         8.4  2.200000          1\n",
      "\n",
      "üî¨ Liste des features extraites :\n",
      "   - COARSE\n",
      "   - SAND\n",
      "   - SILT\n",
      "   - CLAY\n",
      "   - TEXTURE_USDA\n",
      "   - TEXTURE_SOTER\n",
      "   - BULK\n",
      "   - REF_BULK\n",
      "   - ORG_CARBON\n",
      "   - PH_WATER\n",
      "   - TOTAL_N\n",
      "   - CN_RATIO\n",
      "   - CEC_SOIL\n",
      "   - CEC_CLAY\n",
      "   - CEC_EFF\n",
      "   - TEB\n",
      "   - BSAT\n",
      "   - ALUM_SAT\n",
      "   - ESP\n",
      "   - TCARBON_EQ\n",
      "   - GYPSUM\n",
      "   - ELEC_COND\n",
      "\n",
      "üåæ Statistiques des propri√©t√©s num√©riques :\n",
      "\n",
      "   COARSE          : min=   -9.00 | max=   46.00 | mean=    8.61\n",
      "   SAND            : min=   -9.00 | max=   90.00 | mean=   44.27\n",
      "   SILT            : min=   -9.00 | max=   46.00 | mean=   30.48\n",
      "   CLAY            : min=   -9.00 | max=   55.00 | mean=   19.50\n",
      "   BULK            : min=   -9.00 | max=    1.76 | mean=    1.14\n",
      "   REF_BULK        : min=    1.20 | max=    2.03 | mean=    1.71\n",
      "   ORG_CARBON      : min=   -9.00 | max=    5.92 | mean=    0.54\n",
      "   PH_WATER        : min=   -9.00 | max=    8.60 | mean=    7.32\n",
      "   TOTAL_N         : min=   -9.00 | max=    3.69 | mean=    0.61\n",
      "   CN_RATIO        : min=   -9.00 | max=   15.00 | mean=    8.74\n",
      "   CEC_SOIL        : min=   -9.00 | max=   41.00 | mean=   13.71\n",
      "   CEC_CLAY        : min=   -9.00 | max=   83.00 | mean=   56.63\n",
      "   CEC_EFF         : min=   -9.00 | max=  143.00 | mean=   36.88\n",
      "   TEB             : min=   -9.00 | max=  143.00 | mean=   36.70\n",
      "   BSAT            : min=   -9.00 | max=  100.00 | mean=   92.01\n",
      "   ALUM_SAT        : min=   -9.00 | max=   40.00 | mean=   -0.17\n",
      "   ESP             : min=   -9.00 | max=   67.00 | mean=    4.90\n",
      "   TCARBON_EQ      : min=   -9.00 | max=   27.90 | mean=   10.39\n",
      "   GYPSUM          : min=   -9.00 | max=   57.60 | mean=    3.20\n",
      "   ELEC_COND       : min=   -9.00 | max=   32.00 | mean=    1.44\n",
      "\n",
      "üó∫Ô∏è √âtendue g√©ographique :\n",
      "   Longitude : -2.9375¬∞ √† 11.5125¬∞\n",
      "   Latitude  : 32.0042¬∞ √† 37.3125¬∞\n",
      "   Zone      : NORD uniquement (lat > 32.0¬∞N)\n",
      "\n",
      "‚úÖ Traitement termin√© avec succ√®s !\n",
      "\n",
      "üìÅ Fichier de sortie : C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\HWSD2_D1_points.csv\n",
      "\n",
      "üí° R√âSUM√â DU TRAITEMENT :\n",
      "   ‚Ä¢ Filtrage g√©ographique : latitude > 32.0¬∞N\n",
      "   ‚Ä¢ √âchantillonnage spatial : 1 point/10 pixels\n",
      "   ‚Ä¢ √âchantillonnage al√©atoire : max 25,000 points\n",
      "   ‚Ä¢ Format : longitude, latitude + attributs de sol\n",
      "   ‚Ä¢ Pr√™t pour fusion avec fire data et machine learning !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION DES CHEMINS\n",
    "# ============================================================================\n",
    "sol_path = r\"C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\"\n",
    "mdb_path = os.path.join(sol_path, \"HWSD2.mdb\")\n",
    "bil_path = os.path.join(sol_path, \"HWSD2.bil\")\n",
    "shp_path = r\"C:\\Users\\pc\\Desktop\\DM\\datasets\\shapefiles\\algerie_tunisie.shp\"\n",
    "output_dir = os.path.join(sol_path, \"output\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTRACTION DONN√âES DE SOL HWSD2 - FORMAT POINTS (LAT/LON)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 1 : CHARGER LE SHAPEFILE\n",
    "# ============================================================================\n",
    "print(\"\\n[1/6] Chargement du shapefile...\")\n",
    "\n",
    "# Essayer plusieurs m√©thodes pour √©viter les probl√®mes numpy\n",
    "try:\n",
    "    import fiona\n",
    "    with fiona.open(shp_path) as src:\n",
    "        geoms = [feature['geometry'] for feature in src]\n",
    "        crs = src.crs\n",
    "    \n",
    "    # Cr√©er un GeoDataFrame manuellement\n",
    "    import shapely.geometry\n",
    "    from shapely.geometry import shape\n",
    "    \n",
    "    geometries = [shape(geom) for geom in geoms]\n",
    "    gdf_zone = gpd.GeoDataFrame({'geometry': geometries}, crs=crs)\n",
    "    \n",
    "    print(f\"   ‚úì Shapefile charg√© : {len(gdf_zone)} entit√©s\")\n",
    "    print(f\"   ‚úì CRS : {gdf_zone.crs}\")\n",
    "    \n",
    "except Exception as e1:\n",
    "    print(f\"   ‚ö† M√©thode 1 √©chou√©e : {e1}\")\n",
    "    print(\"   üîÑ Tentative m√©thode alternative...\")\n",
    "    \n",
    "    try:\n",
    "        # M√©thode alternative sans geopandas\n",
    "        gdf_zone = gpd.read_file(shp_path, engine='pyogrio')\n",
    "        print(f\"   ‚úì Shapefile charg√© : {len(gdf_zone)} entit√©s\")\n",
    "        print(f\"   ‚úì CRS : {gdf_zone.crs}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"   ‚úó Erreur : {e2}\")\n",
    "        raise\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 2 : EXTRAIRE LES DONN√âES D1 DE LA BASE MDB\n",
    "# ============================================================================\n",
    "print(\"\\n[2/6] Extraction des donn√©es de HWSD2.mdb (LAYER = D1)...\")\n",
    "\n",
    "conn_str = (\n",
    "    r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'\n",
    "    f'DBQ={mdb_path};'\n",
    ")\n",
    "\n",
    "try:\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    print(\"   ‚úì Connexion √† la base de donn√©es √©tablie\")\n",
    "    \n",
    "    colonnes = [\n",
    "        \"HWSD2_SMU_ID\",\n",
    "        \"COARSE\", \"SAND\", \"SILT\", \"CLAY\", \n",
    "        \"TEXTURE_USDA\", \"TEXTURE_SOTER\", \n",
    "        \"BULK\", \"REF_BULK\", \"ORG_CARBON\", \"PH_WATER\",\n",
    "        \"TOTAL_N\", \"CN_RATIO\", \"CEC_SOIL\", \"CEC_CLAY\", \n",
    "        \"CEC_EFF\", \"TEB\", \"BSAT\", \"ALUM_SAT\", \"ESP\", \n",
    "        \"TCARBON_EQ\", \"GYPSUM\", \"ELEC_COND\"\n",
    "    ]\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT {', '.join(colonnes)}\n",
    "    FROM HWSD2_LAYERS\n",
    "    WHERE LAYER = 'D1'\n",
    "    \"\"\"\n",
    "    \n",
    "    df_sol_D1 = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"   ‚úì Donn√©es extraites : {len(df_sol_D1)} enregistrements (couche D1)\")\n",
    "    print(f\"   ‚úì Features extraites : {len(colonnes)-1} attributs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚úó Erreur : {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 3 : CLIPPER LE RASTER HWSD2.bil\n",
    "# ============================================================================\n",
    "print(\"\\n[3/6] Clippage du raster HWSD2.bil...\")\n",
    "\n",
    "with rasterio.open(bil_path) as src:\n",
    "    print(f\"   ‚úì Raster ouvert : {src.width} x {src.height} pixels\")\n",
    "    print(f\"   ‚úì CRS raster : {src.crs}\")\n",
    "    \n",
    "    if gdf_zone.crs != src.crs:\n",
    "        print(f\"   ‚ö† Reprojection du shapefile vers {src.crs}\")\n",
    "        gdf_zone = gdf_zone.to_crs(src.crs)\n",
    "    \n",
    "    shapes_geom = [feature[\"geometry\"] for feature in gdf_zone.__geo_interface__[\"features\"]]\n",
    "    out_image, out_transform = mask(src, shapes_geom, crop=True, nodata=src.nodata)\n",
    "    out_meta = src.meta.copy()\n",
    "    \n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": out_image.shape[1],\n",
    "        \"width\": out_image.shape[2],\n",
    "        \"transform\": out_transform,\n",
    "        \"compress\": \"lzw\"\n",
    "    })\n",
    "    \n",
    "    raster_clip_path = os.path.join(output_dir, \"HWSD2_clipped.tif\")\n",
    "    with rasterio.open(raster_clip_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "    \n",
    "    print(f\"   ‚úì Raster clipp√© sauvegard√©\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 4 : EXTRAIRE LES POINTS (LONGITUDE, LATITUDE) DU RASTER\n",
    "# ============================================================================\n",
    "print(\"\\n[4/6] Extraction des points (longitude, latitude) du raster...\")\n",
    "\n",
    "# Param√®tres de filtrage et √©chantillonnage\n",
    "LATITUDE_MIN = 32.0   # Garder uniquement le Nord (> 32¬∞N)\n",
    "SAMPLING_STEP = 10    # √âTAPE 1: Prendre 1 pixel tous les N pixels (spatial uniforme)\n",
    "\n",
    "print(f\"   üìç Filtrage : latitude > {LATITUDE_MIN}¬∞N (zone Nord)\")\n",
    "print(f\"   üéØ √âchantillonnage : 1 point tous les {SAMPLING_STEP} pixels\")\n",
    "\n",
    "with rasterio.open(raster_clip_path) as src:\n",
    "    image = src.read(1)\n",
    "    transform = src.transform\n",
    "    \n",
    "    # Cr√©er les listes pour stocker les donn√©es\n",
    "    longitudes = []\n",
    "    latitudes = []\n",
    "    hwsd_ids = []\n",
    "    \n",
    "    # Parcourir chaque pixel non-nul\n",
    "    rows, cols = np.where(image != src.nodata)\n",
    "    \n",
    "    print(f\"   ‚è≥ Extraction de {len(rows):,} pixels valides...\")\n",
    "    \n",
    "    # Compteurs pour statistiques\n",
    "    total_pixels = len(rows)\n",
    "    pixels_sampled = 0\n",
    "    pixels_north = 0\n",
    "    \n",
    "    for idx, (row, col) in enumerate(zip(rows, cols)):\n",
    "        # √âCHANTILLONNAGE : prendre 1 pixel tous les SAMPLING_STEP\n",
    "        if idx % SAMPLING_STEP != 0:\n",
    "            continue\n",
    "        \n",
    "        pixels_sampled += 1\n",
    "        \n",
    "        # R√©cup√©rer la valeur HWSD2_SMU_ID\n",
    "        hwsd_id = int(image[row, col])\n",
    "        \n",
    "        # Convertir les coordonn√©es pixel en coordonn√©es g√©ographiques\n",
    "        lon, lat = rasterio.transform.xy(transform, row, col, offset='center')\n",
    "        \n",
    "        # FILTRAGE : garder uniquement latitude > LATITUDE_MIN\n",
    "        if lat < LATITUDE_MIN:\n",
    "            continue\n",
    "        \n",
    "        pixels_north += 1\n",
    "        \n",
    "        longitudes.append(lon)\n",
    "        latitudes.append(lat)\n",
    "        hwsd_ids.append(hwsd_id)\n",
    "    \n",
    "    print(f\"   ‚úì Pixels totaux           : {total_pixels:,}\")\n",
    "    print(f\"   ‚úì Apr√®s √©chantillonnage   : {pixels_sampled:,}\")\n",
    "    print(f\"   ‚úì Points Nord (lat>{LATITUDE_MIN}¬∞) : {pixels_north:,}\")\n",
    "    print(f\"   ‚úì Taux de r√©duction       : {(1 - pixels_north/total_pixels)*100:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 5 : √âCHANTILLONNAGE AL√âATOIRE FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n[5/6] √âchantillonnage al√©atoire final...\")\n",
    "\n",
    "# Nombre final de points souhait√©s (AJUSTABLE)\n",
    "FINAL_SAMPLE_SIZE = 25000  # Tu peux changer : 20000, 25000, 30000, etc.\n",
    "\n",
    "# Cr√©er DataFrame temporaire\n",
    "df_points_temp = pd.DataFrame({\n",
    "    'longitude': longitudes,\n",
    "    'latitude': latitudes,\n",
    "    'HWSD2_SMU_ID': hwsd_ids\n",
    "})\n",
    "\n",
    "print(f\"   üìä Points avant √©chantillonnage final : {len(df_points_temp):,}\")\n",
    "\n",
    "# Si on a plus de points que souhait√©, √©chantillonner\n",
    "if len(df_points_temp) > FINAL_SAMPLE_SIZE:\n",
    "    print(f\"   üé≤ √âchantillonnage al√©atoire vers {FINAL_SAMPLE_SIZE:,} points...\")\n",
    "    \n",
    "    # √âchantillonnage al√©atoire simple avec seed fixe (reproductible)\n",
    "    df_points = df_points_temp.sample(n=FINAL_SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"   ‚úì √âchantillonnage termin√© : {len(df_points):,} points\")\n",
    "    print(f\"   ‚úì Types de sol uniques    : {df_points['HWSD2_SMU_ID'].nunique():,}\")\n",
    "    print(f\"   ‚úì R√©duction totale        : {(1 - len(df_points)/total_pixels)*100:.2f}%\")\n",
    "else:\n",
    "    df_points = df_points_temp.copy()\n",
    "    print(f\"   ‚úì Pas besoin d'√©chantillonnage (d√©j√† < {FINAL_SAMPLE_SIZE:,} points)\")\n",
    "\n",
    "print(f\"   ‚úì Dataset final : {len(df_points):,} points\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 6 : JOINTURE AVEC LES ATTRIBUTS D1\n",
    "# ============================================================================\n",
    "print(\"\\n[6/6] Jointure avec les attributs D1...\")\n",
    "\n",
    "# Jointure sur HWSD2_SMU_ID\n",
    "df_final = df_points.merge(df_sol_D1, on='HWSD2_SMU_ID', how='left')\n",
    "\n",
    "print(f\"   ‚ö† Lignes apr√®s jointure   : {len(df_final):,}\")\n",
    "\n",
    "# PROBL√àME : Certains HWSD2_SMU_ID ont plusieurs enregistrements dans D1\n",
    "# SOLUTION : Garder uniquement la premi√®re occurrence (ou faire une agr√©gation)\n",
    "n_before = len(df_final)\n",
    "df_final = df_final.drop_duplicates(subset=['longitude', 'latitude'], keep='first')\n",
    "n_after = len(df_final)\n",
    "\n",
    "if n_before != n_after:\n",
    "    print(f\"   üîß D√©doublonnage effectu√© : {n_before:,} ‚Üí {n_after:,} lignes\")\n",
    "    print(f\"   ‚úì {n_before - n_after:,} doublons supprim√©s\")\n",
    "\n",
    "# Supprimer HWSD2_SMU_ID (pas demand√© dans le r√©sultat final)\n",
    "df_final = df_final.drop(columns=['HWSD2_SMU_ID'])\n",
    "\n",
    "print(f\"   ‚úì Jointure effectu√©e : {len(df_final):,} points uniques\")\n",
    "print(f\"   ‚úì Colonnes finales : {len(df_final.columns)} (longitude, latitude + {len(df_final.columns)-2} attributs)\")\n",
    "\n",
    "# V√©rifier les valeurs manquantes\n",
    "nb_manquants = df_final['SAND'].isna().sum()\n",
    "if nb_manquants > 0:\n",
    "    print(f\"   ‚ö† {nb_manquants} points sans attributs D1\")\n",
    "\n",
    "# R√©organiser les colonnes : longitude, latitude en premier\n",
    "cols = ['longitude', 'latitude'] + [col for col in df_final.columns if col not in ['longitude', 'latitude']]\n",
    "df_final = df_final[cols]\n",
    "\n",
    "# ============================================================================\n",
    "# SAUVEGARDE DES R√âSULTATS\n",
    "# ============================================================================\n",
    "print(\"\\n[Sauvegarde] Exportation des r√©sultats...\")\n",
    "\n",
    "# Sauvegarder uniquement le CSV\n",
    "csv_output = os.path.join(output_dir, \"HWSD2_D1_points.csv\")\n",
    "df_final.to_csv(csv_output, index=False, encoding='utf-8-sig')\n",
    "print(f\"   ‚úì CSV sauvegard√© : {csv_output}\")\n",
    "print(f\"   ‚úì Taille du fichier : {len(df_final):,} lignes √ó {len(df_final.columns)} colonnes\")\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTIQUES FINALES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"R√âSUM√â DES DONN√âES (Couche D1 - 0-20cm de profondeur)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìç Zone d'√©tude : {len(df_final):,} points\")\n",
    "print(f\"üìä Features extraites : {len(df_final.columns)-2} attributs\")\n",
    "\n",
    "print(\"\\nüìã Aper√ßu des premi√®res lignes :\")\n",
    "print(df_final.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nüî¨ Liste des features extraites :\")\n",
    "for col in df_final.columns:\n",
    "    if col not in ['longitude', 'latitude']:\n",
    "        print(f\"   - {col}\")\n",
    "\n",
    "print(\"\\nüåæ Statistiques des propri√©t√©s num√©riques :\\n\")\n",
    "proprietes = ['COARSE', 'SAND', 'SILT', 'CLAY', 'BULK', 'REF_BULK', \n",
    "              'ORG_CARBON', 'PH_WATER', 'TOTAL_N', 'CN_RATIO', \n",
    "              'CEC_SOIL', 'CEC_CLAY', 'CEC_EFF', 'TEB', 'BSAT', \n",
    "              'ALUM_SAT', 'ESP', 'TCARBON_EQ', 'GYPSUM', 'ELEC_COND']\n",
    "\n",
    "for prop in proprietes:\n",
    "    if prop in df_final.columns:\n",
    "        valeurs = df_final[prop].dropna()\n",
    "        if len(valeurs) > 0:\n",
    "            print(f\"   {prop:15s} : min={valeurs.min():8.2f} | \"\n",
    "                  f\"max={valeurs.max():8.2f} | mean={valeurs.mean():8.2f}\")\n",
    "\n",
    "# Statistiques sur les coordonn√©es\n",
    "print(\"\\nüó∫Ô∏è √âtendue g√©ographique :\")\n",
    "print(f\"   Longitude : {df_final['longitude'].min():.4f}¬∞ √† {df_final['longitude'].max():.4f}¬∞\")\n",
    "print(f\"   Latitude  : {df_final['latitude'].min():.4f}¬∞ √† {df_final['latitude'].max():.4f}¬∞\")\n",
    "print(f\"   Zone      : NORD uniquement (lat > {LATITUDE_MIN}¬∞N)\")\n",
    "\n",
    "print(\"\\n‚úÖ Traitement termin√© avec succ√®s !\")\n",
    "print(f\"\\nüìÅ Fichier de sortie : {csv_output}\")\n",
    "print(\"\\nüí° R√âSUM√â DU TRAITEMENT :\")\n",
    "print(f\"   ‚Ä¢ Filtrage g√©ographique : latitude > {LATITUDE_MIN}¬∞N\")\n",
    "print(f\"   ‚Ä¢ √âchantillonnage spatial : 1 point/{SAMPLING_STEP} pixels\")\n",
    "print(f\"   ‚Ä¢ √âchantillonnage al√©atoire : max {FINAL_SAMPLE_SIZE:,} points\")\n",
    "print(\"   ‚Ä¢ Format : longitude, latitude + attributs de sol\")\n",
    "print(\"   ‚Ä¢ Pr√™t pour fusion avec fire data et machine learning !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "201209d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ CHARGEMENT ET VISUALISATION DES DONN√âES\n",
      "================================================================================\n",
      "\n",
      "[√âtape 1/7] Chargement des donn√©es...\n",
      "   ‚úÖ Donn√©es charg√©es: 25,000 lignes √ó 24 colonnes\n",
      "   ‚úÖ Fichier: C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\HWSD2_D1_points.csv\n",
      "\n",
      "[√âtape 2/7] Configuration...\n",
      "   ‚úÖ Dossier de sortie: C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\output\\visualisations\n",
      "   ‚úÖ Configuration termin√©e\n",
      "\n",
      "[√âtape 3/7] Dashboard des valeurs manquantes...\n",
      "   ‚úÖ 01_valeurs_manquantes.png\n",
      "\n",
      "[√âtape 4/7] Matrice de corr√©lation...\n",
      "   ‚úÖ 02_correlations.png\n",
      "\n",
      "[√âtape 5/7] Histogrammes des distributions...\n",
      "   ‚úÖ 03_distributions.png\n",
      "\n",
      "[√âtape 6/7] Boxplots...\n",
      "   ‚úÖ 04_boxplots.png\n",
      "\n",
      "[√âtape 7/7] QQ-Plots pour comparer les distributions...\n",
      "   ‚úÖ 05_qqplots.png\n",
      "\n",
      "================================================================================\n",
      "‚úÖ VISUALISATIONS TERMIN√âES!\n",
      "================================================================================\n",
      "\n",
      "üéâ 5 fichiers g√©n√©r√©s:\n",
      "\n",
      "   ‚úÖ 01_valeurs_manquantes.png (289.7 KB)\n",
      "   ‚úÖ 02_correlations.png (767.7 KB)\n",
      "   ‚úÖ 03_distributions.png (806.9 KB)\n",
      "   ‚úÖ 04_boxplots.png (627.2 KB)\n",
      "   ‚úÖ 05_qqplots.png (466.6 KB)\n",
      "\n",
      "üìÅ Dossier: C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\output\\visualisations\n",
      "\n",
      "üí° Ouvrez le dossier pour voir vos visualisations!\n",
      "\n",
      "üìä Liste compl√®te des visualisations:\n",
      "   1. 01_valeurs_manquantes.png - Dashboard des valeurs manquantes\n",
      "   2. 02_correlations.png - Matrice de corr√©lation et top corr√©lations\n",
      "   3. 03_distributions.png - Histogrammes des distributions\n",
      "   4. 04_boxplots.png - Boxplots pour d√©tecter les outliers\n",
      "   5. 05_qqplots.png - QQ-plots entre variables corr√©l√©es\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CODE COMPLET - VISUALISATIONS POUR HWSD2_D1_points\n",
    "# ============================================================================\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöÄ CHARGEMENT ET VISUALISATION DES DONN√âES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 1 : CHARGEMENT DES DONN√âES\n",
    "# ============================================================================\n",
    "print(\"\\n[√âtape 1/7] Chargement des donn√©es...\")\n",
    "\n",
    "# D√©finir les chemins\n",
    "base_dir = r\"C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\"\n",
    "output_dir = os.path.join(base_dir, \"output\")\n",
    "fichier_csv = os.path.join(base_dir, \"HWSD2_D1_points.csv\")\n",
    "\n",
    "# Cr√©er le dossier de sortie\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Charger les donn√©es\n",
    "try:\n",
    "    df_final = pd.read_csv(fichier_csv)\n",
    "    print(f\"   ‚úÖ Donn√©es charg√©es: {df_final.shape[0]:,} lignes √ó {df_final.shape[1]} colonnes\")\n",
    "    print(f\"   ‚úÖ Fichier: {fichier_csv}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"   ‚ùå ERREUR: Fichier non trouv√©!\")\n",
    "    print(f\"   üìÇ Chemin recherch√©: {fichier_csv}\")\n",
    "    print(\"\\nüí° V√©rifiez que le fichier existe √† cet emplacement\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå ERREUR lors du chargement: {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 2 : CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"\\n[√âtape 2/7] Configuration...\")\n",
    "\n",
    "# Cr√©er le dossier de visualisations\n",
    "viz_dir = os.path.join(output_dir, \"visualisations\")\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# Configuration matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"   ‚úÖ Dossier de sortie: {viz_dir}\")\n",
    "print(f\"   ‚úÖ Configuration termin√©e\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 3 : DASHBOARD DES VALEURS MANQUANTES\n",
    "# ============================================================================\n",
    "print(\"\\n[√âtape 3/7] Dashboard des valeurs manquantes...\")\n",
    "\n",
    "all_cols = [col for col in df_final.columns if col not in ['longitude', 'latitude']]\n",
    "missing_df = pd.DataFrame({\n",
    "    'Variable': all_cols,\n",
    "    'Valeurs manquantes': [df_final[col].isna().sum() for col in all_cols],\n",
    "    'Pourcentage': [(df_final[col].isna().sum() / len(df_final)) * 100 for col in all_cols]\n",
    "}).set_index('Variable').sort_values('Pourcentage', ascending=False)\n",
    "\n",
    "try:\n",
    "    fig = plt.figure(figsize=(16, max(10, len(all_cols) * 0.4)))\n",
    "    gs = fig.add_gridspec(1, 2, width_ratios=[2, 1])\n",
    "    fig.suptitle('DASHBOARD DES VALEURS MANQUANTES', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Barplot\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    colors = ['#d73027' if x > 50 else '#fee090' if x > 10 else '#91cf60' \n",
    "              for x in missing_df['Pourcentage']]\n",
    "    \n",
    "    bars = ax1.barh(range(len(missing_df)), missing_df['Pourcentage'], \n",
    "                    color=colors, edgecolor='black', linewidth=1.2)\n",
    "    ax1.set_yticks(range(len(missing_df)))\n",
    "    ax1.set_yticklabels(missing_df.index, fontsize=9)\n",
    "    ax1.set_xlabel('Pourcentage de valeurs manquantes (%)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Valeurs manquantes par variable', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (bar, val) in enumerate(zip(bars, missing_df['Pourcentage'])):\n",
    "        if val > 0:\n",
    "            ax1.text(val + 0.5, i, f'{val:.1f}%', va='center', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax1.axvline(10, color='orange', linestyle='--', linewidth=2, label='10%')\n",
    "    ax1.axvline(50, color='red', linestyle='--', linewidth=2, label='50%')\n",
    "    ax1.legend(fontsize=10)\n",
    "    \n",
    "    # Stats\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    total_cells = len(df_final) * len(all_cols)\n",
    "    total_missing = int(missing_df['Valeurs manquantes'].sum())\n",
    "    pct_global = (total_missing / total_cells) * 100\n",
    "    \n",
    "    stats_text = f\"\"\"STATISTIQUES GLOBALES\n",
    "\n",
    "üìä Observations: {len(df_final):,}\n",
    "üìä Variables: {len(all_cols)}\n",
    "üìä Cellules: {total_cells:,}\n",
    "\n",
    "‚ùå Manquantes: {total_missing:,}\n",
    "üìâ Taux: {pct_global:.2f}%\n",
    "‚úÖ Compl√©tude: {100-pct_global:.2f}%\n",
    "\n",
    "TOP 5 MANQUANTS:\"\"\"\n",
    "    \n",
    "    for i, (idx, row) in enumerate(missing_df.head(5).iterrows(), 1):\n",
    "        if row['Valeurs manquantes'] > 0:\n",
    "            stats_text += f\"\\n{i}. {idx}: {row['Pourcentage']:.1f}%\"\n",
    "    \n",
    "    ax2.text(0.05, 0.5, stats_text, fontsize=10, va='center', family='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, '01_valeurs_manquantes.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   ‚úÖ 01_valeurs_manquantes.png\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur: {e}\")\n",
    "    plt.close('all')\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 4 : MATRICE DE CORR√âLATION\n",
    "# ============================================================================\n",
    "print(\"\\n[√âtape 4/7] Matrice de corr√©lation...\")\n",
    "\n",
    "try:\n",
    "    numeric_cols = [col for col in df_final.select_dtypes(include=[np.number]).columns \n",
    "                    if col not in ['longitude', 'latitude']]\n",
    "    numeric_data = df_final[numeric_cols].dropna(thresh=len(df_final)*0.5, axis=1)\n",
    "\n",
    "    if len(numeric_data.columns) > 1:\n",
    "        corr_matrix = numeric_data.corr()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        fig.suptitle('MATRICE DE CORR√âLATION', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Heatmap\n",
    "        im = axes[0].imshow(corr_matrix.values, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
    "        \n",
    "        for i in range(len(corr_matrix)):\n",
    "            for j in range(len(corr_matrix.columns)):\n",
    "                if i > j:\n",
    "                    axes[0].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "        \n",
    "        axes[0].set_xticks(range(len(corr_matrix)))\n",
    "        axes[0].set_yticks(range(len(corr_matrix)))\n",
    "        axes[0].set_xticklabels(corr_matrix.columns, rotation=90, ha='right')\n",
    "        axes[0].set_yticklabels(corr_matrix.columns)\n",
    "        axes[0].set_title('Corr√©lation (Pearson)', fontweight='bold')\n",
    "        plt.colorbar(im, ax=axes[0])\n",
    "        \n",
    "        # Top corr√©lations\n",
    "        corr_pairs = corr_matrix.unstack()\n",
    "        corr_pairs = corr_pairs[corr_pairs < 1].sort_values(ascending=False)\n",
    "        top_corr = corr_pairs.head(15)\n",
    "        \n",
    "        colors = ['#d73027' if abs(x) > 0.7 else '#fee090' if abs(x) > 0.4 else '#91cf60' \n",
    "                  for x in top_corr.values]\n",
    "        \n",
    "        axes[1].barh(range(len(top_corr)), top_corr.values, color=colors, edgecolor='black')\n",
    "        axes[1].set_yticks(range(len(top_corr)))\n",
    "        axes[1].set_yticklabels([f\"{p[0]} ‚Üî {p[1]}\" for p in top_corr.index], fontsize=9)\n",
    "        axes[1].set_xlabel('Corr√©lation', fontweight='bold')\n",
    "        axes[1].set_title('Top 15 corr√©lations', fontweight='bold')\n",
    "        axes[1].axvline(0.7, color='red', linestyle='--', linewidth=1, label='Forte (0.7)')\n",
    "        axes[1].axvline(-0.7, color='red', linestyle='--', linewidth=1)\n",
    "        axes[1].axvline(0.4, color='orange', linestyle='--', linewidth=1, label='Mod√©r√©e (0.4)')\n",
    "        axes[1].axvline(-0.4, color='orange', linestyle='--', linewidth=1)\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, '02_correlations.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"   ‚úÖ 02_correlations.png\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Pas assez de variables num√©riques\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur: {e}\")\n",
    "    plt.close('all')\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 5 : DISTRIBUTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n[√âtape 5/7] Histogrammes des distributions...\")\n",
    "\n",
    "try:\n",
    "    main_props = ['SAND', 'SILT', 'CLAY', 'BULK', 'ORG_CARBON', 'PH_WATER', \n",
    "                  'CEC_SOIL', 'TOTAL_N', 'CN_RATIO', 'COARSE', 'REF_BULK', 'TEB']\n",
    "    available_props = [p for p in main_props if p in df_final.columns]\n",
    "\n",
    "    if available_props:\n",
    "        n_cols = 3\n",
    "        n_rows = (len(available_props) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))\n",
    "        fig.suptitle('DISTRIBUTIONS DES PROPRI√âT√âS DU SOL', fontsize=16, fontweight='bold')\n",
    "        axes = axes.flatten() if len(available_props) > 1 else [axes]\n",
    "        \n",
    "        for idx, prop in enumerate(available_props):\n",
    "            ax = axes[idx]\n",
    "            data = df_final[prop].dropna()\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                ax.hist(data.values, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "                \n",
    "                mean_val = float(data.mean())\n",
    "                median_val = float(data.median())\n",
    "                ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, \n",
    "                          label=f'Moy: {mean_val:.2f}')\n",
    "                ax.axvline(median_val, color='green', linestyle='--', linewidth=2,\n",
    "                          label=f'M√©d: {median_val:.2f}')\n",
    "                \n",
    "                ax.set_xlabel(prop, fontweight='bold')\n",
    "                ax.set_ylabel('Fr√©quence', fontweight='bold')\n",
    "                ax.set_title(f'{prop}', fontweight='bold')\n",
    "                ax.legend(fontsize=8)\n",
    "                ax.grid(alpha=0.3)\n",
    "                \n",
    "                std = float(data.std())\n",
    "                n = len(data)\n",
    "                skew = float(((data - mean_val) ** 3).sum() / (n * std ** 3)) if std > 0 else 0\n",
    "                \n",
    "                stats = f'n={n}\\nstd={std:.2f}\\nskew={skew:.2f}'\n",
    "                ax.text(0.98, 0.97, stats, transform=ax.transAxes, va='top', ha='right',\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=9)\n",
    "        \n",
    "        for idx in range(len(available_props), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, '03_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"   ‚úÖ 03_distributions.png\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur: {e}\")\n",
    "    plt.close('all')\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 6 : BOXPLOTS\n",
    "# ============================================================================\n",
    "print(\"\\n[√âtape 6/7] Boxplots...\")\n",
    "\n",
    "try:\n",
    "    if available_props:\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))\n",
    "        fig.suptitle('BOXPLOTS - D√âTECTION DES OUTLIERS', fontsize=16, fontweight='bold')\n",
    "        axes = axes.flatten() if len(available_props) > 1 else [axes]\n",
    "        \n",
    "        for idx, prop in enumerate(available_props):\n",
    "            ax = axes[idx]\n",
    "            data = df_final[prop].dropna()\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                bp = ax.boxplot([data.values], patch_artist=True, widths=0.6,\n",
    "                               boxprops=dict(facecolor='lightblue', edgecolor='black'),\n",
    "                               medianprops=dict(color='red', linewidth=2))\n",
    "                \n",
    "                Q1 = float(data.quantile(0.25))\n",
    "                Q3 = float(data.quantile(0.75))\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                ax.set_ylabel(prop, fontweight='bold')\n",
    "                ax.set_title(f'{prop}', fontweight='bold')\n",
    "                ax.set_xticks([])\n",
    "                ax.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                stats = (f'Min: {float(data.min()):.2f}\\nQ1: {Q1:.2f}\\n'\n",
    "                        f'M√©d: {float(data.median()):.2f}\\nQ3: {Q3:.2f}\\n'\n",
    "                        f'Max: {float(data.max()):.2f}\\nIQR: {IQR:.2f}')\n",
    "                ax.text(1.15, 0.5, stats, transform=ax.transAxes, va='center',\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=9)\n",
    "        \n",
    "        for idx in range(len(available_props), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, '04_boxplots.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"   ‚úÖ 04_boxplots.png\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur: {e}\")\n",
    "    plt.close('all')\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 7 : QQ-PLOTS (comparaison entre variables corr√©l√©es)\n",
    "# ============================================================================\n",
    "print(\"\\n[√âtape 7/7] QQ-Plots pour comparer les distributions...\")\n",
    "\n",
    "try:\n",
    "    if len(numeric_data.columns) > 1 and 'corr_matrix' in locals():\n",
    "        # Trouver les 6 paires les plus corr√©l√©es\n",
    "        corr_pairs_qq = corr_matrix.unstack()\n",
    "        corr_pairs_qq = corr_pairs_qq[corr_pairs_qq < 1]\n",
    "        top_pairs = corr_pairs_qq.abs().sort_values(ascending=False).head(6).index.tolist()\n",
    "        \n",
    "        n_pairs = len(top_pairs)\n",
    "        n_cols_qq = 3\n",
    "        n_rows_qq = (n_pairs + n_cols_qq - 1) // n_cols_qq\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows_qq, n_cols_qq, figsize=(18, 5*n_rows_qq))\n",
    "        fig.suptitle('QQ-PLOTS - COMPARAISON DES DISTRIBUTIONS', fontsize=16, fontweight='bold')\n",
    "        axes = axes.flatten() if n_pairs > 1 else [axes]\n",
    "        \n",
    "        for idx, (var1, var2) in enumerate(top_pairs):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            data1 = df_final[var1].dropna()\n",
    "            data2 = df_final[var2].dropna()\n",
    "            \n",
    "            if len(data1) > 10 and len(data2) > 10:\n",
    "                # Trier les donn√©es\n",
    "                data1_sorted = np.sort(data1.values)\n",
    "                data2_sorted = np.sort(data2.values)\n",
    "                \n",
    "                # Interpoler pour avoir la m√™me longueur\n",
    "                n = min(len(data1_sorted), len(data2_sorted))\n",
    "                quantiles1 = np.percentile(data1_sorted, np.linspace(0, 100, n))\n",
    "                quantiles2 = np.percentile(data2_sorted, np.linspace(0, 100, n))\n",
    "                \n",
    "                # Cr√©er le QQ-plot\n",
    "                ax.scatter(quantiles1, quantiles2, alpha=0.6, s=20, \n",
    "                          color='steelblue', edgecolor='black')\n",
    "                \n",
    "                # Ligne de r√©f√©rence (identit√©)\n",
    "                min_val = min(float(quantiles1.min()), float(quantiles2.min()))\n",
    "                max_val = max(float(quantiles1.max()), float(quantiles2.max()))\n",
    "                ax.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
    "                       linewidth=2, label='Ligne identit√©')\n",
    "                \n",
    "                ax.set_xlabel(f'{var1}', fontweight='bold')\n",
    "                ax.set_ylabel(f'{var2}', fontweight='bold')\n",
    "                ax.set_title(f'QQ-Plot: {var1} vs {var2}', fontweight='bold', pad=10)\n",
    "                ax.grid(alpha=0.3)\n",
    "                ax.legend(fontsize=8)\n",
    "                \n",
    "                # Afficher le coefficient de corr√©lation\n",
    "                corr_val = float(corr_matrix.loc[var1, var2])\n",
    "                ax.text(0.05, 0.95, f'r = {corr_val:.3f}', transform=ax.transAxes,\n",
    "                       va='top', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "                       fontsize=10, fontweight='bold')\n",
    "        \n",
    "        # Masquer les axes vides\n",
    "        for idx in range(n_pairs, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, '05_qqplots.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"   ‚úÖ 05_qqplots.png\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Pas assez de variables pour cr√©er des QQ-plots\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    plt.close('all')\n",
    "\n",
    "# ============================================================================\n",
    "# RAPPORT FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ VISUALISATIONS TERMIN√âES!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "files = sorted([f for f in os.listdir(viz_dir) if f.endswith('.png')])\n",
    "if files:\n",
    "    print(f\"\\nüéâ {len(files)} fichiers g√©n√©r√©s:\\n\")\n",
    "    for f in files:\n",
    "        size = os.path.getsize(os.path.join(viz_dir, f)) / 1024\n",
    "        print(f\"   ‚úÖ {f} ({size:.1f} KB)\")\n",
    "    print(f\"\\nüìÅ Dossier: {viz_dir}\")\n",
    "    print(f\"\\nüí° Ouvrez le dossier pour voir vos visualisations!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Aucun fichier g√©n√©r√© - v√©rifiez les erreurs ci-dessus\")\n",
    "\n",
    "print(\"\\nüìä Liste compl√®te des visualisations:\")\n",
    "print(\"   1. 01_valeurs_manquantes.png - Dashboard des valeurs manquantes\")\n",
    "print(\"   2. 02_correlations.png - Matrice de corr√©lation et top corr√©lations\")\n",
    "print(\"   3. 03_distributions.png - Histogrammes des distributions\")\n",
    "print(\"   4. 04_boxplots.png - Boxplots pour d√©tecter les outliers\")\n",
    "print(\"   5. 05_qqplots.png - QQ-plots entre variables corr√©l√©es\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda4426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NETTOYAGE DES DONN√âES DE SOL HWSD2\n",
      "================================================================================\n",
      "\n",
      "[1/6] Chargement des donn√©es...\n",
      "   ‚úì Donn√©es charg√©es : 25000 enregistrements, 24 colonnes\n",
      "\n",
      "[2/6] Identification des types de colonnes...\n",
      "   ‚úì Colonnes num√©riques : 20\n",
      "   ‚úì Colonnes cat√©gorielles : 2\n",
      "   ‚úì Colonnes g√©ographiques : 2\n",
      "\n",
      "   üìä Valeurs manquantes avant nettoyage :\n",
      "      TEXTURE_USDA         :  1269 (  5.1%)\n",
      "      REF_BULK             :  1269 (  5.1%)\n",
      "\n",
      "[3/6] Traitement des valeurs cod√©es -9 (valeurs manquantes)...\n",
      "   ‚úì COARSE               :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì SAND                 :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì SILT                 :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì CLAY                 :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì BULK                 :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì ORG_CARBON           :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì PH_WATER             :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì TOTAL_N              :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì CN_RATIO             :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì CEC_SOIL             :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì CEC_CLAY             :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì CEC_EFF              :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì TEB                  :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì BSAT                 :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì ALUM_SAT             :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì ESP                  :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì TCARBON_EQ           :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì GYPSUM               :   116 valeurs -9 remplac√©es par NaN\n",
      "   ‚úì ELEC_COND            :   116 valeurs -9 remplac√©es par NaN\n",
      "\n",
      "[4/6] Imputation des valeurs manquantes...\n",
      "\n",
      "   üî¢ Imputation des colonnes num√©riques avec KNN...\n",
      "      ‚ÑπÔ∏è  20 colonnes num√©riques avec valeurs manquantes\n",
      "      ‚è≥ Application de KNN Imputer...\n",
      "      ‚úì KNN Imputation termin√©e pour 20 colonnes\n",
      "         COARSE               : 116 ‚Üí 0 valeurs manquantes\n",
      "         SAND                 : 116 ‚Üí 0 valeurs manquantes\n",
      "         SILT                 : 116 ‚Üí 0 valeurs manquantes\n",
      "         CLAY                 : 116 ‚Üí 0 valeurs manquantes\n",
      "         BULK                 : 116 ‚Üí 0 valeurs manquantes\n",
      "         REF_BULK             : 1269 ‚Üí 0 valeurs manquantes\n",
      "         ORG_CARBON           : 116 ‚Üí 0 valeurs manquantes\n",
      "         PH_WATER             : 116 ‚Üí 0 valeurs manquantes\n",
      "         TOTAL_N              : 116 ‚Üí 0 valeurs manquantes\n",
      "         CN_RATIO             : 116 ‚Üí 0 valeurs manquantes\n",
      "         CEC_SOIL             : 116 ‚Üí 0 valeurs manquantes\n",
      "         CEC_CLAY             : 116 ‚Üí 0 valeurs manquantes\n",
      "         CEC_EFF              : 116 ‚Üí 0 valeurs manquantes\n",
      "         TEB                  : 116 ‚Üí 0 valeurs manquantes\n",
      "         BSAT                 : 116 ‚Üí 0 valeurs manquantes\n",
      "         ALUM_SAT             : 116 ‚Üí 0 valeurs manquantes\n",
      "         ESP                  : 116 ‚Üí 0 valeurs manquantes\n",
      "         TCARBON_EQ           : 116 ‚Üí 0 valeurs manquantes\n",
      "         GYPSUM               : 116 ‚Üí 0 valeurs manquantes\n",
      "         ELEC_COND            : 116 ‚Üí 0 valeurs manquantes\n",
      "\n",
      "   üè∑Ô∏è  Imputation des colonnes cat√©gorielles avec KNN...\n",
      "\n",
      "      üìä Traitement de TEXTURE_USDA...\n",
      "         Valeurs manquantes avant : 1269 (5.1%)\n",
      "         ‚úì Valeurs manquantes apr√®s  : 0 (0.0%)\n",
      "\n",
      "      ‚úì TEXTURE_SOTER : Aucune valeur manquante\n",
      "\n",
      "   ‚úÖ Imputation compl√®te (num√©riques + cat√©gorielles avec KNN)\n",
      "\n",
      "[5/6] Gestion des outliers avec transformation logarithmique...\n",
      "\n",
      "   üìä Colonnes avec outliers (top 10) :\n",
      "      ELEC_COND            :  8166 outliers ( 32.7%)\n",
      "      ORG_CARBON           :  7332 outliers ( 29.3%)\n",
      "      TEB                  :  6254 outliers ( 25.0%)\n",
      "      CEC_EFF              :  6025 outliers ( 24.1%)\n",
      "      BULK                 :  5663 outliers ( 22.7%)\n",
      "      PH_WATER             :  5525 outliers ( 22.1%)\n",
      "      ESP                  :  4961 outliers ( 19.8%)\n",
      "      BSAT                 :  4387 outliers ( 17.5%)\n",
      "      GYPSUM               :  3372 outliers ( 13.5%)\n",
      "      CLAY                 :  3256 outliers ( 13.0%)\n",
      "\n",
      "   üîÑ Application de log(x+1) sur 18 colonnes...\n",
      "      ‚ö†Ô∏è  ELEC_COND: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì ELEC_COND: transformation log appliqu√©e ‚Üí ELEC_COND_log\n",
      "      ‚ö†Ô∏è  ORG_CARBON: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì ORG_CARBON: transformation log appliqu√©e ‚Üí ORG_CARBON_log\n",
      "      ‚ö†Ô∏è  TEB: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì TEB: transformation log appliqu√©e ‚Üí TEB_log\n",
      "      ‚ö†Ô∏è  CEC_EFF: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì CEC_EFF: transformation log appliqu√©e ‚Üí CEC_EFF_log\n",
      "      ‚ö†Ô∏è  BULK: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì BULK: transformation log appliqu√©e ‚Üí BULK_log\n",
      "      ‚ö†Ô∏è  PH_WATER: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì PH_WATER: transformation log appliqu√©e ‚Üí PH_WATER_log\n",
      "      ‚ö†Ô∏è  ESP: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì ESP: transformation log appliqu√©e ‚Üí ESP_log\n",
      "      ‚ö†Ô∏è  BSAT: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì BSAT: transformation log appliqu√©e ‚Üí BSAT_log\n",
      "      ‚ö†Ô∏è  GYPSUM: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì GYPSUM: transformation log appliqu√©e ‚Üí GYPSUM_log\n",
      "      ‚ö†Ô∏è  CLAY: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì CLAY: transformation log appliqu√©e ‚Üí CLAY_log\n",
      "      ‚ö†Ô∏è  TCARBON_EQ: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì TCARBON_EQ: transformation log appliqu√©e ‚Üí TCARBON_EQ_log\n",
      "      ‚ö†Ô∏è  TOTAL_N: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì TOTAL_N: transformation log appliqu√©e ‚Üí TOTAL_N_log\n",
      "      ‚ö†Ô∏è  CEC_SOIL: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì CEC_SOIL: transformation log appliqu√©e ‚Üí CEC_SOIL_log\n",
      "      ‚ö†Ô∏è  CN_RATIO: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì CN_RATIO: transformation log appliqu√©e ‚Üí CN_RATIO_log\n",
      "      ‚ö†Ô∏è  SILT: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì SILT: transformation log appliqu√©e ‚Üí SILT_log\n",
      "      ‚ö†Ô∏è  SAND: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì SAND: transformation log appliqu√©e ‚Üí SAND_log\n",
      "      ‚úì REF_BULK: transformation log appliqu√©e ‚Üí REF_BULK_log\n",
      "      ‚ö†Ô∏è  ALUM_SAT: d√©calage de 6.00 appliqu√©\n",
      "      ‚úì ALUM_SAT: transformation log appliqu√©e ‚Üí ALUM_SAT_log\n",
      "\n",
      "[6/6] Encoding des variables cat√©gorielles...\n",
      "\n",
      "   üìä Traitement de TEXTURE_USDA...\n",
      "      ‚ÑπÔ∏è  6 valeurs uniques d√©tect√©es\n",
      "      ‚úì TEXTURE_USDA: 6 cat√©gories encod√©es\n",
      "      üìã Aper√ßu du mapping (5 premi√®res valeurs):\n",
      "         10.0                           ‚Üí 0\n",
      "         11.0                           ‚Üí 1\n",
      "         12.0                           ‚Üí 2\n",
      "         3.0                            ‚Üí 3\n",
      "         5.0                            ‚Üí 4\n",
      "         ... et 1 autres cat√©gories\n",
      "\n",
      "   üìä Traitement de TEXTURE_SOTER...\n",
      "      ‚ÑπÔ∏è  4 valeurs uniques d√©tect√©es\n",
      "      ‚úì TEXTURE_SOTER: 4 cat√©gories encod√©es\n",
      "      üìã Aper√ßu du mapping (5 premi√®res valeurs):\n",
      "         -                              ‚Üí 0\n",
      "         C                              ‚Üí 1\n",
      "         F                              ‚Üí 2\n",
      "         M                              ‚Üí 3\n",
      "\n",
      "   ‚úÖ Encoding des variables cat√©gorielles termin√©\n",
      "\n",
      "   üóëÔ∏è  Suppression des colonnes cat√©gorielles originales...\n",
      "      ‚úì TEXTURE_USDA supprim√©e (remplac√©e par TEXTURE_USDA_encoded)\n",
      "      ‚úì TEXTURE_SOTER supprim√©e (remplac√©e par TEXTURE_SOTER_encoded)\n",
      "\n",
      "[7/9] Suppression des colonnes originales (remplac√©es par log)...\n",
      "   ‚úì ELEC_COND supprim√©e (remplac√©e par ELEC_COND_log)\n",
      "   ‚úì ORG_CARBON supprim√©e (remplac√©e par ORG_CARBON_log)\n",
      "   ‚úì TEB supprim√©e (remplac√©e par TEB_log)\n",
      "   ‚úì CEC_EFF supprim√©e (remplac√©e par CEC_EFF_log)\n",
      "   ‚úì BULK supprim√©e (remplac√©e par BULK_log)\n",
      "   ‚úì PH_WATER supprim√©e (remplac√©e par PH_WATER_log)\n",
      "   ‚úì ESP supprim√©e (remplac√©e par ESP_log)\n",
      "   ‚úì BSAT supprim√©e (remplac√©e par BSAT_log)\n",
      "   ‚úì GYPSUM supprim√©e (remplac√©e par GYPSUM_log)\n",
      "   ‚úì CLAY supprim√©e (remplac√©e par CLAY_log)\n",
      "   ‚úì TCARBON_EQ supprim√©e (remplac√©e par TCARBON_EQ_log)\n",
      "   ‚úì TOTAL_N supprim√©e (remplac√©e par TOTAL_N_log)\n",
      "   ‚úì CEC_SOIL supprim√©e (remplac√©e par CEC_SOIL_log)\n",
      "   ‚úì CN_RATIO supprim√©e (remplac√©e par CN_RATIO_log)\n",
      "   ‚úì SILT supprim√©e (remplac√©e par SILT_log)\n",
      "   ‚úì SAND supprim√©e (remplac√©e par SAND_log)\n",
      "   ‚úì REF_BULK supprim√©e (remplac√©e par REF_BULK_log)\n",
      "   ‚úì ALUM_SAT supprim√©e (remplac√©e par ALUM_SAT_log)\n",
      "   ‚úÖ 18 colonnes originales supprim√©es\n",
      "\n",
      "[8/9] Analyse et suppression des colonnes fortement corr√©l√©es...\n",
      "   ‚ÑπÔ∏è  Analyse de 20 colonnes num√©riques...\n",
      "\n",
      "   üìä Statistiques de corr√©lation :\n",
      "      ‚Ä¢ Colonnes analys√©es : 20\n",
      "      ‚Ä¢ Paires fortement corr√©l√©es (r > 0.85) : 55\n",
      "      ‚Ä¢ Colonnes identifi√©es pour suppression : 13\n",
      "\n",
      "   üìà Top 10 des paires les plus corr√©l√©es :\n",
      "      üóëÔ∏è  BSAT_log             ‚Üî üóëÔ∏è  PH_WATER_log         : r = 0.997\n",
      "      üóëÔ∏è  CEC_EFF_log          ‚Üî ‚úÖ TEB_log              : r = 0.996\n",
      "      üóëÔ∏è  PH_WATER_log         ‚Üî üóëÔ∏è  BULK_log             : r = 0.994\n",
      "      üóëÔ∏è  BSAT_log             ‚Üî üóëÔ∏è  BULK_log             : r = 0.994\n",
      "      üóëÔ∏è  TOTAL_N_log          ‚Üî ‚úÖ ORG_CARBON_log       : r = 0.993\n",
      "      üóëÔ∏è  CN_RATIO_log         ‚Üî üóëÔ∏è  BULK_log             : r = 0.987\n",
      "      üóëÔ∏è  CN_RATIO_log         ‚Üî üóëÔ∏è  TOTAL_N_log          : r = 0.987\n",
      "      üóëÔ∏è  TOTAL_N_log          ‚Üî üóëÔ∏è  BULK_log             : r = 0.987\n",
      "      üóëÔ∏è  CN_RATIO_log         ‚Üî ‚úÖ ORG_CARBON_log       : r = 0.987\n",
      "      üóëÔ∏è  CEC_SOIL_log         ‚Üî üóëÔ∏è  CLAY_log             : r = 0.979\n",
      "\n",
      "   üóëÔ∏è  Suppression de 13 colonnes redondantes :\n",
      "      ‚Ä¢ ALUM_SAT_log                   (7 corr√©lations fortes)\n",
      "      ‚Ä¢ BSAT_log                       (3 corr√©lations fortes)\n",
      "      ‚Ä¢ BULK_log                       (1 corr√©lations fortes)\n",
      "      ‚Ä¢ CEC_EFF_log                    (1 corr√©lations fortes)\n",
      "      ‚Ä¢ CEC_SOIL_log                   (6 corr√©lations fortes)\n",
      "      ‚Ä¢ CLAY_log                       (4 corr√©lations fortes)\n",
      "      ‚Ä¢ CN_RATIO_log                   (7 corr√©lations fortes)\n",
      "      ‚Ä¢ ESP_log                        (1 corr√©lations fortes)\n",
      "      ‚Ä¢ PH_WATER_log                   (2 corr√©lations fortes)\n",
      "      ‚Ä¢ SAND_log                       (6 corr√©lations fortes)\n",
      "      ‚Ä¢ SILT_log                       (10 corr√©lations fortes)\n",
      "      ‚Ä¢ TCARBON_EQ_log                 (2 corr√©lations fortes)\n",
      "      ‚Ä¢ TOTAL_N_log                    (5 corr√©lations fortes)\n",
      "\n",
      "   ‚úÖ 13 colonnes effectivement supprim√©es\n",
      "   üìä Colonnes restantes : 11\n",
      "\n",
      "[9/9] Sauvegarde des donn√©es nettoy√©es...\n",
      "\n",
      "   üìä Statistiques finales :\n",
      "      Enregistrements : 25000\n",
      "      Colonnes totales : 11\n",
      "      Colonnes num√©riques : 7\n",
      "      Colonnes encod√©es : 2\n",
      "      Colonnes g√©ographiques : 2\n",
      "\n",
      "   ‚úÖ Aucune valeur manquante !\n",
      "\n",
      "   üíæ Sauvegarde des fichiers...\n",
      "      ‚úì CSV : C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\cleaned_data\\HWSD2_D1_cleaned.csv\n",
      "      ‚úì Excel : C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\cleaned_data\\HWSD2_D1_cleaned.xlsx\n",
      "      ‚úì Encoders : C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\cleaned_data\\label_encoders.pkl\n",
      "      ‚úì Mapping : C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\cleaned_data\\encoding_mapping.txt\n",
      "\n",
      "================================================================================\n",
      "‚úÖ NETTOYAGE TERMIN√â AVEC SUCC√àS\n",
      "================================================================================\n",
      "\n",
      "üìÅ Fichiers sauvegard√©s dans : C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\\output\\cleaned_data\n",
      "\n",
      "üìä R√âSUM√â :\n",
      "   ‚Ä¢ Valeurs -9 remplac√©es\n",
      "   ‚Ä¢ KNN Imputation appliqu√©e\n",
      "   ‚Ä¢ Transformation log sur 18 colonnes\n",
      "   ‚Ä¢ 2 variables encod√©es\n",
      "   ‚Ä¢ 13 colonnes corr√©l√©es supprim√©es\n",
      "   ‚Ä¢ Dataset final : 25000 √ó 11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"NETTOYAGE DES DONN√âES DE SOL HWSD2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Chemins\n",
    "sol_path = r\"C:\\Users\\pc\\Desktop\\DM\\datasets\\sol\"\n",
    "output_dir = os.path.join(sol_path, \"output\")\n",
    "cleaned_dir = os.path.join(output_dir, \"cleaned_data\")\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "# Charger les donn√©es\n",
    "print(\"\\n[1/6] Chargement des donn√©es...\")\n",
    "# MODIFI√â : Charger depuis CSV au lieu de GeoPackage\n",
    "csv_path = os.path.join(output_dir, \"HWSD2_D1_points.csv\")  # Adaptez le nom du fichier\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"   ‚úì Donn√©es charg√©es : {len(df)} enregistrements, {len(df.columns)} colonnes\")\n",
    "\n",
    "# Sauvegarder une copie des donn√©es originales\n",
    "df_original = df.copy()\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 1 : IDENTIFICATION DES COLONNES PAR TYPE\n",
    "# ============================================================================\n",
    "print(\"\\n[2/6] Identification des types de colonnes...\")\n",
    "\n",
    "# MODIFI√â : Colonnes g√©ographiques au lieu de geometry\n",
    "geo_cols = ['longitude', 'latitude']\n",
    "categorical_cols = ['TEXTURE_USDA', 'TEXTURE_SOTER']\n",
    "numeric_cols = [col for col in df.columns \n",
    "                if col not in categorical_cols + geo_cols]\n",
    "\n",
    "print(f\"   ‚úì Colonnes num√©riques : {len(numeric_cols)}\")\n",
    "print(f\"   ‚úì Colonnes cat√©gorielles : {len(categorical_cols)}\")\n",
    "print(f\"   ‚úì Colonnes g√©ographiques : {len(geo_cols)}\")\n",
    "\n",
    "# Statistiques initiales\n",
    "print(\"\\n   üìä Valeurs manquantes avant nettoyage :\")\n",
    "for col in df.columns:\n",
    "    if col not in geo_cols:\n",
    "        missing = df[col].isna().sum()\n",
    "        if missing > 0:\n",
    "            pct = (missing / len(df)) * 100\n",
    "            print(f\"      {col:20s} : {missing:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 2 : TRAITEMENT DES VALEURS ABERRANTES (-9 = valeur manquante)\n",
    "# ============================================================================\n",
    "print(\"\\n[3/6] Traitement des valeurs cod√©es -9 (valeurs manquantes)...\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        mask = df[col] == -9\n",
    "        n_replaced = mask.sum()\n",
    "        if n_replaced > 0:\n",
    "            df.loc[mask, col] = np.nan\n",
    "            print(f\"   ‚úì {col:20s} : {n_replaced:5d} valeurs -9 remplac√©es par NaN\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 3 : IMPUTATION DES VALEURS MANQUANTES AVEC KNN\n",
    "# ============================================================================\n",
    "print(\"\\n[4/6] Imputation des valeurs manquantes...\")\n",
    "\n",
    "# 3.1 IMPUTATION DES COLONNES NUM√âRIQUES AVEC KNN\n",
    "print(\"\\n   üî¢ Imputation des colonnes num√©riques avec KNN...\")\n",
    "\n",
    "numeric_data = df[numeric_cols].copy()\n",
    "missing_counts = numeric_data.isna().sum()\n",
    "cols_with_missing = missing_counts[missing_counts > 0].index.tolist()\n",
    "\n",
    "if len(cols_with_missing) > 0:\n",
    "    print(f\"      ‚ÑπÔ∏è  {len(cols_with_missing)} colonnes num√©riques avec valeurs manquantes\")\n",
    "    \n",
    "    knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    print(\"      ‚è≥ Application de KNN Imputer...\")\n",
    "    numeric_imputed = knn_imputer.fit_transform(numeric_data)\n",
    "    \n",
    "    numeric_data_imputed = pd.DataFrame(\n",
    "        numeric_imputed, \n",
    "        columns=numeric_cols,\n",
    "        index=numeric_data.index\n",
    "    )\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df[col] = numeric_data_imputed[col]\n",
    "    \n",
    "    print(f\"      ‚úì KNN Imputation termin√©e pour {len(cols_with_missing)} colonnes\")\n",
    "    \n",
    "    for col in cols_with_missing:\n",
    "        original_missing = numeric_data[col].isna().sum()\n",
    "        new_missing = df[col].isna().sum()\n",
    "        print(f\"         {col:20s} : {original_missing} ‚Üí {new_missing} valeurs manquantes\")\n",
    "else:\n",
    "    print(\"      ‚úì Aucune valeur manquante dans les colonnes num√©riques\")\n",
    "\n",
    "# 3.2 IMPUTATION DES COLONNES CAT√âGORIELLES\n",
    "print(\"\\n   üè∑Ô∏è  Imputation des colonnes cat√©gorielles avec KNN...\")\n",
    "\n",
    "categorical_encoders_temp = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(f\"\\n      üìä Traitement de {col}...\")\n",
    "            print(f\"         Valeurs manquantes avant : {missing_count} ({missing_count/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            le_temp = LabelEncoder()\n",
    "            col_data = df[col].copy()\n",
    "            col_data = col_data.astype(str)\n",
    "            col_data = col_data.replace(['nan', 'None', ''], np.nan)\n",
    "            \n",
    "            mask_not_missing = col_data.notna()\n",
    "            values_not_missing = col_data[mask_not_missing]\n",
    "            encoded_values = le_temp.fit_transform(values_not_missing)\n",
    "            \n",
    "            col_encoded = np.full(len(col_data), np.nan)\n",
    "            col_encoded[mask_not_missing] = encoded_values\n",
    "            \n",
    "            temp_df = df[numeric_cols].copy()\n",
    "            temp_df[f'{col}_encoded_temp'] = col_encoded\n",
    "            \n",
    "            knn_cat = KNNImputer(n_neighbors=5, weights='distance')\n",
    "            temp_imputed = knn_cat.fit_transform(temp_df[[f'{col}_encoded_temp']])\n",
    "            \n",
    "            imputed_encoded = np.round(temp_imputed[:, 0]).astype(int)\n",
    "            imputed_categories = le_temp.inverse_transform(imputed_encoded)\n",
    "            \n",
    "            df[col] = imputed_categories\n",
    "            \n",
    "            new_missing = df[col].isna().sum()\n",
    "            print(f\"         ‚úì Valeurs manquantes apr√®s  : {new_missing} ({new_missing/len(df)*100:.1f}%)\")\n",
    "            \n",
    "            categorical_encoders_temp[col] = le_temp\n",
    "        else:\n",
    "            print(f\"\\n      ‚úì {col} : Aucune valeur manquante\")\n",
    "\n",
    "print(\"\\n   ‚úÖ Imputation compl√®te (num√©riques + cat√©gorielles avec KNN)\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 4 : GESTION DES OUTLIERS AVEC TRANSFORMATION LOG\n",
    "# ============================================================================\n",
    "print(\"\\n[5/6] Gestion des outliers avec transformation logarithmique...\")\n",
    "\n",
    "def detect_outliers_iqr(data, col, threshold=1.5):\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    outliers = ((data[col] < lower_bound) | (data[col] > upper_bound)).sum()\n",
    "    return outliers, (outliers / len(data)) * 100\n",
    "\n",
    "outlier_report = []\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        n_outliers, pct_outliers = detect_outliers_iqr(df, col)\n",
    "        if n_outliers > 0:\n",
    "            outlier_report.append({\n",
    "                'column': col,\n",
    "                'n_outliers': n_outliers,\n",
    "                'pct_outliers': pct_outliers\n",
    "            })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_report).sort_values('pct_outliers', ascending=False)\n",
    "cols_to_transform = []\n",
    "\n",
    "if len(outlier_df) > 0:\n",
    "    print(f\"\\n   üìä Colonnes avec outliers (top 10) :\")\n",
    "    for idx, row in outlier_df.head(10).iterrows():\n",
    "        print(f\"      {row['column']:20s} : {row['n_outliers']:5d} outliers ({row['pct_outliers']:5.1f}%)\")\n",
    "    \n",
    "    cols_to_transform = outlier_df[outlier_df['pct_outliers'] > 5]['column'].tolist()\n",
    "    \n",
    "    if len(cols_to_transform) > 0:\n",
    "        print(f\"\\n   üîÑ Application de log(x+1) sur {len(cols_to_transform)} colonnes...\")\n",
    "        \n",
    "        for col in cols_to_transform:\n",
    "            min_val = df[col].min()\n",
    "            \n",
    "            if min_val < 0:\n",
    "                df[col] = df[col] - min_val + 1\n",
    "                print(f\"      ‚ö†Ô∏è  {col}: d√©calage de {-min_val + 1:.2f} appliqu√©\")\n",
    "            \n",
    "            df[f'{col}_log'] = np.log1p(df[col])\n",
    "            print(f\"      ‚úì {col}: transformation log appliqu√©e ‚Üí {col}_log\")\n",
    "        \n",
    "        numeric_cols.extend([f'{col}_log' for col in cols_to_transform])\n",
    "else:\n",
    "    print(\"   ‚úì Aucun outlier significatif d√©tect√©\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 5 : ENCODING DES VARIABLES CAT√âGORIELLES\n",
    "# ============================================================================\n",
    "print(\"\\n[6/6] Encoding des variables cat√©gorielles...\")\n",
    "\n",
    "encoders = {}\n",
    "encoded_mapping = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n   üìä Traitement de {col}...\")\n",
    "        \n",
    "        df[f'{col}_encoded'] = df[col].copy()\n",
    "        df[f'{col}_encoded'] = df[f'{col}_encoded'].astype(str)\n",
    "        df[f'{col}_encoded'] = df[f'{col}_encoded'].replace(['nan', 'None', ''], 'Unknown')\n",
    "        \n",
    "        unique_values = df[f'{col}_encoded'].unique()\n",
    "        print(f\"      ‚ÑπÔ∏è  {len(unique_values)} valeurs uniques d√©tect√©es\")\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        df[f'{col}_encoded'] = le.fit_transform(df[f'{col}_encoded'])\n",
    "        \n",
    "        encoders[col] = le\n",
    "        mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        encoded_mapping[col] = mapping\n",
    "        \n",
    "        print(f\"      ‚úì {col}: {len(le.classes_)} cat√©gories encod√©es\")\n",
    "        print(f\"      üìã Aper√ßu du mapping (5 premi√®res valeurs):\")\n",
    "        for i, (original, encoded) in enumerate(list(mapping.items())[:5]):\n",
    "            print(f\"         {original:30s} ‚Üí {encoded}\")\n",
    "        \n",
    "        if len(mapping) > 5:\n",
    "            print(f\"         ... et {len(mapping) - 5} autres cat√©gories\")\n",
    "\n",
    "print(\"\\n   ‚úÖ Encoding des variables cat√©gorielles termin√©\")\n",
    "\n",
    "print(\"\\n   üóëÔ∏è  Suppression des colonnes cat√©gorielles originales...\")\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "        print(f\"      ‚úì {col} supprim√©e (remplac√©e par {col}_encoded)\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 6 : SUPPRESSION DES COLONNES ORIGINALES TRANSFORM√âES EN LOG\n",
    "# ============================================================================\n",
    "print(\"\\n[7/9] Suppression des colonnes originales (remplac√©es par log)...\")\n",
    "\n",
    "if len(cols_to_transform) > 0:\n",
    "    for col in cols_to_transform:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "            print(f\"   ‚úì {col} supprim√©e (remplac√©e par {col}_log)\")\n",
    "    print(f\"   ‚úÖ {len(cols_to_transform)} colonnes originales supprim√©es\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è  Aucune colonne transform√©e en log\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 7 : SUPPRESSION DES COLONNES FORTEMENT CORR√âL√âES\n",
    "# ============================================================================\n",
    "print(\"\\n[8/9] Analyse et suppression des colonnes fortement corr√©l√©es...\")\n",
    "\n",
    "current_numeric_cols = [col for col in df.columns \n",
    "                       if col not in categorical_cols + geo_cols\n",
    "                       and not col.endswith('_encoded')]\n",
    "\n",
    "print(f\"   ‚ÑπÔ∏è  Analyse de {len(current_numeric_cols)} colonnes num√©riques...\")\n",
    "\n",
    "numeric_df = df[current_numeric_cols]\n",
    "corr_matrix = numeric_df.corr().abs()\n",
    "\n",
    "correlation_threshold = 0.85\n",
    "\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "cols_to_drop = set()\n",
    "correlation_pairs = []\n",
    "\n",
    "for column in upper_triangle.columns:\n",
    "    highly_correlated = upper_triangle.index[upper_triangle[column] > correlation_threshold].tolist()\n",
    "    \n",
    "    for corr_col in highly_correlated:\n",
    "        correlation_pairs.append({\n",
    "            'col1': column,\n",
    "            'col2': corr_col,\n",
    "            'correlation': upper_triangle.loc[corr_col, column]\n",
    "        })\n",
    "        \n",
    "        count_col = (upper_triangle[column] > correlation_threshold).sum()\n",
    "        count_corr = (upper_triangle[corr_col] > correlation_threshold).sum()\n",
    "        \n",
    "        if count_col >= count_corr:\n",
    "            cols_to_drop.add(column)\n",
    "        else:\n",
    "            cols_to_drop.add(corr_col)\n",
    "\n",
    "cols_to_drop = list(cols_to_drop)\n",
    "\n",
    "print(f\"\\n   üìä Statistiques de corr√©lation :\")\n",
    "print(f\"      ‚Ä¢ Colonnes analys√©es : {len(current_numeric_cols)}\")\n",
    "print(f\"      ‚Ä¢ Paires fortement corr√©l√©es (r > {correlation_threshold}) : {len(correlation_pairs)}\")\n",
    "print(f\"      ‚Ä¢ Colonnes identifi√©es pour suppression : {len(cols_to_drop)}\")\n",
    "\n",
    "if len(correlation_pairs) > 0:\n",
    "    print(f\"\\n   üìà Top 10 des paires les plus corr√©l√©es :\")\n",
    "    corr_df = pd.DataFrame(correlation_pairs).sort_values('correlation', ascending=False)\n",
    "    for idx, row in corr_df.head(10).iterrows():\n",
    "        marker1 = \"üóëÔ∏è \" if row['col1'] in cols_to_drop else \"‚úÖ\"\n",
    "        marker2 = \"üóëÔ∏è \" if row['col2'] in cols_to_drop else \"‚úÖ\"\n",
    "        print(f\"      {marker1} {row['col1']:20s} ‚Üî {marker2} {row['col2']:20s} : r = {row['correlation']:.3f}\")\n",
    "\n",
    "if len(cols_to_drop) > 0:\n",
    "    print(f\"\\n   üóëÔ∏è  Suppression de {len(cols_to_drop)} colonnes redondantes :\")\n",
    "    \n",
    "    for col in sorted(cols_to_drop):\n",
    "        n_corr = (upper_triangle[col] > correlation_threshold).sum()\n",
    "        print(f\"      ‚Ä¢ {col:30s} ({n_corr} corr√©lations fortes)\")\n",
    "    \n",
    "    cols_actually_dropped = []\n",
    "    for col in cols_to_drop:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "            cols_actually_dropped.append(col)\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ {len(cols_actually_dropped)} colonnes effectivement supprim√©es\")\n",
    "    print(f\"   üìä Colonnes restantes : {len(df.columns)}\")\n",
    "    \n",
    "    cols_to_drop = cols_actually_dropped\n",
    "else:\n",
    "    print(f\"\\n   ‚úÖ Aucune colonne fortement corr√©l√©e d√©tect√©e (seuil r > {correlation_threshold})\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 8 : SAUVEGARDE\n",
    "# ============================================================================\n",
    "print(\"\\n[9/9] Sauvegarde des donn√©es nettoy√©es...\")\n",
    "\n",
    "print(\"\\n   üìä Statistiques finales :\")\n",
    "print(f\"      Enregistrements : {len(df)}\")\n",
    "print(f\"      Colonnes totales : {len(df.columns)}\")\n",
    "print(f\"      Colonnes num√©riques : {len([c for c in df.columns if c not in categorical_cols + geo_cols and not c.endswith('_encoded')])}\")\n",
    "print(f\"      Colonnes encod√©es : {len([c for c in df.columns if c.endswith('_encoded')])}\")\n",
    "print(f\"      Colonnes g√©ographiques : {len(geo_cols)}\")\n",
    "\n",
    "remaining_missing = df.drop(columns=geo_cols).isna().sum()\n",
    "cols_with_missing_final = remaining_missing[remaining_missing > 0]\n",
    "\n",
    "if len(cols_with_missing_final) > 0:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Valeurs manquantes restantes :\")\n",
    "    for col, count in cols_with_missing_final.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"      {col:20s} : {count:5d} ({pct:5.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n   ‚úÖ Aucune valeur manquante !\")\n",
    "\n",
    "print(\"\\n   üíæ Sauvegarde des fichiers...\")\n",
    "\n",
    "# CSV\n",
    "csv_cleaned = os.path.join(cleaned_dir, \"HWSD2_D1_cleaned.csv\")\n",
    "df.to_csv(csv_cleaned, index=False, encoding='utf-8-sig')\n",
    "print(f\"      ‚úì CSV : {csv_cleaned}\")\n",
    "\n",
    "# Excel\n",
    "try:\n",
    "    excel_cleaned = os.path.join(cleaned_dir, \"HWSD2_D1_cleaned.xlsx\")\n",
    "    df.to_excel(excel_cleaned, index=False, engine='openpyxl')\n",
    "    print(f\"      ‚úì Excel : {excel_cleaned}\")\n",
    "except Exception as e:\n",
    "    print(f\"      ‚ö†Ô∏è  Excel non cr√©√© : {e}\")\n",
    "\n",
    "# Encoders\n",
    "import pickle\n",
    "encoders_path = os.path.join(cleaned_dir, \"label_encoders.pkl\")\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "print(f\"      ‚úì Encoders : {encoders_path}\")\n",
    "\n",
    "# Mapping\n",
    "mapping_path = os.path.join(cleaned_dir, \"encoding_mapping.txt\")\n",
    "with open(mapping_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"MAPPING DES VARIABLES CAT√âGORIELLES\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    for col, mapping in encoded_mapping.items():\n",
    "        f.write(f\"{col}:\\n\")\n",
    "        f.write(f\"  Nombre de cat√©gories: {len(mapping)}\\n\\n\")\n",
    "        for original, encoded in sorted(mapping.items(), key=lambda x: x[1]):\n",
    "            f.write(f\"   {original:30s} ‚Üí {encoded}\\n\")\n",
    "        f.write(\"\\n\" + \"-\" * 80 + \"\\n\\n\")\n",
    "print(f\"      ‚úì Mapping : {mapping_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ NETTOYAGE TERMIN√â AVEC SUCC√àS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìÅ Fichiers sauvegard√©s dans : {cleaned_dir}\")\n",
    "print(\"\\nüìä R√âSUM√â :\")\n",
    "print(f\"   ‚Ä¢ Valeurs -9 remplac√©es\")\n",
    "print(f\"   ‚Ä¢ KNN Imputation appliqu√©e\")\n",
    "if len(cols_to_transform) > 0:\n",
    "    print(f\"   ‚Ä¢ Transformation log sur {len(cols_to_transform)} colonnes\")\n",
    "print(f\"   ‚Ä¢ {len(categorical_cols)} variables encod√©es\")\n",
    "if len(cols_to_drop) > 0:\n",
    "    print(f\"   ‚Ä¢ {len(cols_to_drop)} colonnes corr√©l√©es supprim√©es\")\n",
    "print(f\"   ‚Ä¢ Dataset final : {len(df)} √ó {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21027333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FUSION SPATIALE : FIRE + SOIL (K-NN + IDW)\n",
      "================================================================================\n",
      "\n",
      "[1/7] Chargement des donn√©es...\n",
      "   üìç Chargement du dataset FIRE...\n",
      "      ‚úì Fire charg√© : 140,165 points\n",
      "\n",
      "   üåç Chargement du dataset SOIL...\n",
      "      ‚úì Soil charg√© : 25,000 points\n",
      "\n",
      "   üìã Colonnes FIRE : ['latitude', 'longitude', 'class']...\n",
      "   üìã Colonnes SOIL : ['longitude', 'latitude', 'COARSE', 'CEC_CLAY', 'ELEC_COND_log']...\n",
      "\n",
      "[2/7] Identification des types de colonnes SOIL...\n",
      "   üìä Colonnes num√©riques (IDW)       : 9\n",
      "   üìã Colonnes cat√©gorielles (K-NN)   : 0\n",
      "      Exemples num√©riques : ['COARSE', 'CEC_CLAY', 'ELEC_COND_log', 'ORG_CARBON_log', 'TEB_log']\n",
      "\n",
      "[3/7] Application K-NN pour tous les points FIRE...\n",
      "   üîÑ Recherche des 5 plus proches points SOIL...\n",
      "      ‚úì K-NN termin√©\n",
      "      ‚Ä¢ Distance min (1er voisin) : 0.000083¬∞ (~0.01 km)\n",
      "      ‚Ä¢ Distance max (1er voisin) : 0.104215¬∞ (~11.57 km)\n",
      "      ‚Ä¢ Distance moyenne          : 0.023043¬∞ (~2.56 km)\n",
      "\n",
      "[4/7] Classification des points (seuil = 0.1¬∞)...\n",
      "   üìç Points avec match direct     : 140,164 (100.0%)\n",
      "   üî¢ Points n√©cessitant interpolation : 1 (0.0%)\n",
      "\n",
      "[5/7] Attribution des propri√©t√©s SOIL...\n",
      "\n",
      "   üî§ Attribution cat√©gorielles (plus proche voisin)...\n",
      "      ‚úì 0 colonnes cat√©gorielles assign√©es\n",
      "\n",
      "   üî¢ Interpolation num√©riques (IDW avec K=5, power=2)...\n",
      "      ‚úì 9 colonnes num√©riques interpol√©es\n",
      "\n",
      "[6/7] Ajout des m√©tadonn√©es...\n",
      "      ‚úì M√©tadonn√©es ajout√©es\n",
      "        ‚Ä¢ assigned_method : type d'attribution\n",
      "        ‚Ä¢ knn_distance    : distance en degr√©s\n",
      "        ‚Ä¢ knn_distance_km : distance en kilom√®tres\n",
      "\n",
      "[7/7] R√©organisation des colonnes et sauvegarde...\n",
      "   ‚úì Colonnes r√©organis√©es\n",
      "     'class' en derni√®re position\n",
      "\n",
      "üíæ Sauvegarde des fichiers...\n",
      "   ‚úì CSV : C:\\Users\\pc\\Desktop\\DM\\datasets\\merged_fire_soil\\fire_soil_merged_knn_idw.csv\n",
      "   ‚úì Excel : C:\\Users\\pc\\Desktop\\DM\\datasets\\merged_fire_soil\\fire_soil_merged_knn_idw.xlsx\n",
      "   ‚úì Rapport : C:\\Users\\pc\\Desktop\\DM\\datasets\\merged_fire_soil\\fusion_statistics.txt\n",
      "\n",
      "================================================================================\n",
      "‚úÖ FUSION TERMIN√âE AVEC SUCC√àS\n",
      "================================================================================\n",
      "\n",
      "üìä STATISTIQUES FINALES :\n",
      "   ‚Ä¢ Total points FIRE d'origine  : 140,165\n",
      "   ‚Ä¢ Points dans le dataset final : 140,165\n",
      "   ‚Ä¢ Taux de conservation         : 100.0%\n",
      "\n",
      "üìç M√âTHODES D'ATTRIBUTION :\n",
      "   ‚Ä¢ DIRECT_MATCH              : 140,164 points (100.0%)\n",
      "   ‚Ä¢ KNN_IDW_INTERPOLATION     :      1 points (  0.0%)\n",
      "\n",
      "üìè DISTANCES K-NN :\n",
      "   ‚Ä¢ Distance minimale   : 0.000083¬∞ (~0.01 km)\n",
      "   ‚Ä¢ Distance maximale   : 0.104215¬∞ (~11.57 km)\n",
      "   ‚Ä¢ Distance moyenne    : 0.023043¬∞ (~2.56 km)\n",
      "   ‚Ä¢ Distance m√©diane    : 0.021510¬∞ (~2.39 km)\n",
      "\n",
      "üî¢ INTERPOLATION IDW :\n",
      "   ‚Ä¢ Colonnes interpol√©es : 9\n",
      "   ‚Ä¢ Colonnes cat√©gorielles : 0\n",
      "   ‚Ä¢ Nombre de voisins    : 5\n",
      "   ‚Ä¢ Puissance IDW        : 2\n",
      "\n",
      "üéØ COLONNES DU DATASET FINAL :\n",
      "   ‚Ä¢ Total colonnes : 15\n",
      "\n",
      "üëÄ APER√áU DES DONN√âES (5 premi√®res lignes) :\n",
      "================================================================================\n",
      "   latitude  longitude assigned_method  knn_distance_km  class\n",
      "0  36.74886    6.25409    DIRECT_MATCH         4.994383      1\n",
      "1  35.87978    4.44782    DIRECT_MATCH         1.147541      1\n",
      "2  35.70751    5.53337    DIRECT_MATCH         2.647404      1\n",
      "3  32.27667    3.98647    DIRECT_MATCH         2.068412      1\n",
      "4  32.40079    4.00642    DIRECT_MATCH         4.130320      1\n",
      "\n",
      "‚ú® CARACT√âRISTIQUES :\n",
      "   ‚úÖ 100% des points FIRE conserv√©s\n",
      "   ‚úÖ Match direct pour points proches (<0.1¬∞)\n",
      "   ‚úÖ Interpolation IDW pour points √©loign√©s\n",
      "   ‚úÖ Cat√©gorielles : K-NN (plus proche voisin)\n",
      "   ‚úÖ Num√©riques : IDW (interpolation pond√©r√©e)\n",
      "   ‚úÖ Tra√ßabilit√© via m√©tadonn√©es\n",
      "   ‚úÖ Colonne 'class' en derni√®re position\n",
      "\n",
      "================================================================================\n",
      "üöÄ DATASET COMPLET PR√äT POUR LA MOD√âLISATION !\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"FUSION SPATIALE : FIRE + SOIL (K-NN + IDW)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Chemins des fichiers\n",
    "base_path = r\"C:\\Users\\pc\\Desktop\\DM\\datasets\"\n",
    "soil_path = os.path.join(base_path, \"sol\\output\\cleaned_data\", \"HWSD2_D1_cleaned.csv\")\n",
    "fire_path = os.path.join(base_path, \"fire_data\", \"fire_nonfire_north_unbalanced.csv\")\n",
    "output_dir = os.path.join(base_path, \"merged_fire_soil\")\n",
    "\n",
    "# Param√®tres K-NN et IDW\n",
    "K_NEIGHBORS = 5  # Nombre de voisins pour K-NN\n",
    "POWER_IDW = 2    # Puissance pour Inverse Distance Weighting\n",
    "DISTANCE_THRESHOLD = 0.1  # Seuil de distance (degr√©s) pour consid√©rer un match direct\n",
    "\n",
    "# Cr√©er le dossier de sortie\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 1 : CHARGER LES DONN√âES\n",
    "# ============================================================================\n",
    "print(\"\\n[1/7] Chargement des donn√©es...\")\n",
    "\n",
    "print(\"   üìç Chargement du dataset FIRE...\")\n",
    "df_fire = pd.read_csv(fire_path)\n",
    "print(f\"      ‚úì Fire charg√© : {len(df_fire):,} points\")\n",
    "\n",
    "print(\"\\n   üåç Chargement du dataset SOIL...\")\n",
    "df_soil = pd.read_csv(soil_path)\n",
    "print(f\"      ‚úì Soil charg√© : {len(df_soil):,} points\")\n",
    "\n",
    "# V√©rifier les colonnes\n",
    "print(f\"\\n   üìã Colonnes FIRE : {df_fire.columns.tolist()[:5]}...\")\n",
    "print(f\"   üìã Colonnes SOIL : {df_soil.columns.tolist()[:5]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 2 : IDENTIFIER COLONNES NUM√âRIQUES VS CAT√âGORIELLES\n",
    "# ============================================================================\n",
    "print(\"\\n[2/7] Identification des types de colonnes SOIL...\")\n",
    "\n",
    "# Colonnes SOIL √† copier (exclure longitude/latitude)\n",
    "soil_cols_to_copy = [col for col in df_soil.columns \n",
    "                     if col not in ['longitude', 'latitude']]\n",
    "\n",
    "# Identifier les colonnes num√©riques et cat√©gorielles\n",
    "numeric_cols = []\n",
    "categorical_cols = []\n",
    "\n",
    "for col in soil_cols_to_copy:\n",
    "    if df_soil[col].dtype in [np.float64, np.float32, np.int64, np.int32, np.int16, np.int8]:\n",
    "        numeric_cols.append(col)\n",
    "    else:\n",
    "        categorical_cols.append(col)\n",
    "\n",
    "print(f\"   üìä Colonnes num√©riques (IDW)       : {len(numeric_cols)}\")\n",
    "print(f\"   üìã Colonnes cat√©gorielles (K-NN)   : {len(categorical_cols)}\")\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    print(f\"      Exemples num√©riques : {numeric_cols[:5]}\")\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"      Exemples cat√©gorielles : {categorical_cols[:5]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 3 : APPLIQUER K-NN POUR TOUS LES POINTS\n",
    "# ============================================================================\n",
    "print(\"\\n[3/7] Application K-NN pour tous les points FIRE...\")\n",
    "\n",
    "# Pr√©parer les donn√©es pour K-NN\n",
    "X_train = df_soil[['longitude', 'latitude']].values  # Points SOIL\n",
    "X_test = df_fire[['longitude', 'latitude']].values   # Points FIRE\n",
    "\n",
    "print(f\"   üîÑ Recherche des {K_NEIGHBORS} plus proches points SOIL...\")\n",
    "\n",
    "# K-NN pour trouver les k plus proches points SOIL\n",
    "knn = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric='euclidean')\n",
    "knn.fit(X_train)\n",
    "\n",
    "# Trouver les K plus proches voisins\n",
    "distances, indices = knn.kneighbors(X_test)\n",
    "\n",
    "print(f\"      ‚úì K-NN termin√©\")\n",
    "print(f\"      ‚Ä¢ Distance min (1er voisin) : {distances[:, 0].min():.6f}¬∞ (~{distances[:, 0].min()*111:.2f} km)\")\n",
    "print(f\"      ‚Ä¢ Distance max (1er voisin) : {distances[:, 0].max():.6f}¬∞ (~{distances[:, 0].max()*111:.2f} km)\")\n",
    "print(f\"      ‚Ä¢ Distance moyenne          : {distances[:, 0].mean():.6f}¬∞ (~{distances[:, 0].mean()*111:.2f} km)\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 4 : CLASSIFICATION DES POINTS (DIRECT MATCH VS INTERPOLATION)\n",
    "# ============================================================================\n",
    "print(f\"\\n[4/7] Classification des points (seuil = {DISTANCE_THRESHOLD}¬∞)...\")\n",
    "\n",
    "# Points avec match direct (distance < seuil)\n",
    "direct_match_mask = distances[:, 0] < DISTANCE_THRESHOLD\n",
    "n_direct = direct_match_mask.sum()\n",
    "n_interpolated = len(df_fire) - n_direct\n",
    "\n",
    "print(f\"   üìç Points avec match direct     : {n_direct:,} ({n_direct/len(df_fire)*100:.1f}%)\")\n",
    "print(f\"   üî¢ Points n√©cessitant interpolation : {n_interpolated:,} ({n_interpolated/len(df_fire)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 5 : ATTRIBUTION DES PROPRI√âT√âS SOIL\n",
    "# ============================================================================\n",
    "print(f\"\\n[5/7] Attribution des propri√©t√©s SOIL...\")\n",
    "\n",
    "# Cr√©er une copie de df_fire pour y ajouter les colonnes SOIL\n",
    "df_merged = df_fire.copy()\n",
    "\n",
    "# --- 1. COLONNES CAT√âGORIELLES : Plus proche voisin ---\n",
    "print(f\"\\n   üî§ Attribution cat√©gorielles (plus proche voisin)...\")\n",
    "closest_indices = indices[:, 0]  # Index du plus proche voisin\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_merged[col] = df_soil.iloc[closest_indices][col].values\n",
    "    \n",
    "print(f\"      ‚úì {len(categorical_cols)} colonnes cat√©gorielles assign√©es\")\n",
    "\n",
    "# --- 2. COLONNES NUM√âRIQUES : Interpolation IDW ---\n",
    "print(f\"\\n   üî¢ Interpolation num√©riques (IDW avec K={K_NEIGHBORS}, power={POWER_IDW})...\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    interpolated_values = []\n",
    "    \n",
    "    for i in range(len(df_fire)):\n",
    "        neighbor_indices = indices[i]\n",
    "        neighbor_distances = distances[i]\n",
    "        \n",
    "        # Valeurs des k voisins\n",
    "        neighbor_values = df_soil.iloc[neighbor_indices][col].values\n",
    "        \n",
    "        # Calculer les poids IDW\n",
    "        weights = 1 / (neighbor_distances + 1e-6) ** POWER_IDW\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        # Moyenne pond√©r√©e\n",
    "        interpolated_value = np.sum(weights * neighbor_values)\n",
    "        interpolated_values.append(interpolated_value)\n",
    "    \n",
    "    df_merged[col] = interpolated_values\n",
    "\n",
    "print(f\"      ‚úì {len(numeric_cols)} colonnes num√©riques interpol√©es\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 6 : AJOUTER M√âTADONN√âES\n",
    "# ============================================================================\n",
    "print(f\"\\n[6/7] Ajout des m√©tadonn√©es...\")\n",
    "\n",
    "# M√©thode d'attribution\n",
    "df_merged['assigned_method'] = np.where(\n",
    "    direct_match_mask,\n",
    "    'DIRECT_MATCH',\n",
    "    'KNN_IDW_INTERPOLATION'\n",
    ")\n",
    "\n",
    "# Distance au plus proche voisin\n",
    "df_merged['knn_distance'] = distances[:, 0]\n",
    "\n",
    "# Distance en km (approximation : 1¬∞ ‚âà 111 km)\n",
    "df_merged['knn_distance_km'] = distances[:, 0] * 111\n",
    "\n",
    "print(f\"      ‚úì M√©tadonn√©es ajout√©es\")\n",
    "print(f\"        ‚Ä¢ assigned_method : type d'attribution\")\n",
    "print(f\"        ‚Ä¢ knn_distance    : distance en degr√©s\")\n",
    "print(f\"        ‚Ä¢ knn_distance_km : distance en kilom√®tres\")\n",
    "\n",
    "# ============================================================================\n",
    "# √âTAPE 7 : R√âORGANISATION ET SAUVEGARDE\n",
    "# ============================================================================\n",
    "print(\"\\n[7/7] R√©organisation des colonnes et sauvegarde...\")\n",
    "\n",
    "# R√©organiser les colonnes\n",
    "# Ordre : [lat, lon, soil_features, method, distance, class]\n",
    "all_columns = list(df_merged.columns)\n",
    "\n",
    "base_cols = ['latitude', 'longitude']\n",
    "meta_cols = ['assigned_method', 'knn_distance', 'knn_distance_km']\n",
    "target_col = ['class'] if 'class' in all_columns else []\n",
    "\n",
    "# Colonnes SOIL (tout sauf les colonnes sp√©ciales)\n",
    "soil_cols = [col for col in all_columns \n",
    "             if col not in base_cols + meta_cols + target_col]\n",
    "\n",
    "# Ordre final\n",
    "new_order = base_cols + soil_cols + meta_cols + target_col\n",
    "\n",
    "# Filtrer les colonnes qui existent r√©ellement\n",
    "new_order = [col for col in new_order if col in df_merged.columns]\n",
    "df_merged = df_merged[new_order]\n",
    "\n",
    "print(f\"   ‚úì Colonnes r√©organis√©es\")\n",
    "if 'class' in df_merged.columns:\n",
    "    print(f\"     'class' en derni√®re position\")\n",
    "\n",
    "# Sauvegarder\n",
    "print(f\"\\nüíæ Sauvegarde des fichiers...\")\n",
    "\n",
    "# 1. CSV\n",
    "csv_output = os.path.join(output_dir, \"fire_soil_merged_knn_idw.csv\")\n",
    "df_merged.to_csv(csv_output, index=False, encoding='utf-8-sig')\n",
    "print(f\"   ‚úì CSV : {csv_output}\")\n",
    "\n",
    "# 2. Excel\n",
    "try:\n",
    "    excel_output = os.path.join(output_dir, \"fire_soil_merged_knn_idw.xlsx\")\n",
    "    df_merged.to_excel(excel_output, index=False, engine='openpyxl')\n",
    "    print(f\"   ‚úì Excel : {excel_output}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Excel non cr√©√© : {e}\")\n",
    "\n",
    "# 3. Rapport statistiques\n",
    "stats_output = os.path.join(output_dir, \"fusion_statistics.txt\")\n",
    "with open(stats_output, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"RAPPORT DE FUSION FIRE + SOIL\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"DONN√âES D'ENTR√âE\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Points FIRE : {len(df_fire):,}\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Points SOIL : {len(df_soil):,}\\n\\n\")\n",
    "    \n",
    "    f.write(\"R√âSULTATS\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Total points conserv√©s : {len(df_merged):,} (100%)\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Match direct (<{DISTANCE_THRESHOLD}¬∞) : {n_direct:,} ({n_direct/len(df_fire)*100:.1f}%)\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Interpolation IDW : {n_interpolated:,} ({n_interpolated/len(df_fire)*100:.1f}%)\\n\\n\")\n",
    "    \n",
    "    f.write(\"PARAM√àTRES\\n\")\n",
    "    f.write(f\"  ‚Ä¢ K voisins : {K_NEIGHBORS}\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Puissance IDW : {POWER_IDW}\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Seuil distance : {DISTANCE_THRESHOLD}¬∞ (~{DISTANCE_THRESHOLD*111:.1f} km)\\n\\n\")\n",
    "    \n",
    "    f.write(\"DISTANCES K-NN\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Minimale : {distances[:, 0].min():.6f}¬∞ (~{distances[:, 0].min()*111:.2f} km)\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Maximale : {distances[:, 0].max():.6f}¬∞ (~{distances[:, 0].max()*111:.2f} km)\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Moyenne  : {distances[:, 0].mean():.6f}¬∞ (~{distances[:, 0].mean()*111:.2f} km)\\n\")\n",
    "    f.write(f\"  ‚Ä¢ M√©diane  : {np.median(distances[:, 0]):.6f}¬∞ (~{np.median(distances[:, 0])*111:.2f} km)\\n\\n\")\n",
    "    \n",
    "    f.write(\"COLONNES\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Total : {len(df_merged.columns)}\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Num√©riques interpol√©es : {len(numeric_cols)}\\n\")\n",
    "    f.write(f\"  ‚Ä¢ Cat√©gorielles (K-NN) : {len(categorical_cols)}\\n\")\n",
    "\n",
    "print(f\"   ‚úì Rapport : {stats_output}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RAPPORT FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ FUSION TERMIN√âE AVEC SUCC√àS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìä STATISTIQUES FINALES :\")\n",
    "print(f\"   ‚Ä¢ Total points FIRE d'origine  : {len(df_fire):,}\")\n",
    "print(f\"   ‚Ä¢ Points dans le dataset final : {len(df_merged):,}\")\n",
    "print(f\"   ‚Ä¢ Taux de conservation         : 100.0%\")\n",
    "\n",
    "print(f\"\\nüìç M√âTHODES D'ATTRIBUTION :\")\n",
    "method_counts = df_merged['assigned_method'].value_counts()\n",
    "for method, count in method_counts.items():\n",
    "    pct = count / len(df_merged) * 100\n",
    "    print(f\"   ‚Ä¢ {method:25s} : {count:6,} points ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìè DISTANCES K-NN :\")\n",
    "print(f\"   ‚Ä¢ Distance minimale   : {distances[:, 0].min():.6f}¬∞ (~{distances[:, 0].min()*111:.2f} km)\")\n",
    "print(f\"   ‚Ä¢ Distance maximale   : {distances[:, 0].max():.6f}¬∞ (~{distances[:, 0].max()*111:.2f} km)\")\n",
    "print(f\"   ‚Ä¢ Distance moyenne    : {distances[:, 0].mean():.6f}¬∞ (~{distances[:, 0].mean()*111:.2f} km)\")\n",
    "print(f\"   ‚Ä¢ Distance m√©diane    : {np.median(distances[:, 0]):.6f}¬∞ (~{np.median(distances[:, 0])*111:.2f} km)\")\n",
    "\n",
    "print(f\"\\nüî¢ INTERPOLATION IDW :\")\n",
    "print(f\"   ‚Ä¢ Colonnes interpol√©es : {len(numeric_cols)}\")\n",
    "print(f\"   ‚Ä¢ Colonnes cat√©gorielles : {len(categorical_cols)}\")\n",
    "print(f\"   ‚Ä¢ Nombre de voisins    : {K_NEIGHBORS}\")\n",
    "print(f\"   ‚Ä¢ Puissance IDW        : {POWER_IDW}\")\n",
    "\n",
    "print(f\"\\nüéØ COLONNES DU DATASET FINAL :\")\n",
    "print(f\"   ‚Ä¢ Total colonnes : {len(df_merged.columns)}\")\n",
    "\n",
    "print(f\"\\nüëÄ APER√áU DES DONN√âES (5 premi√®res lignes) :\")\n",
    "print(\"=\" * 80)\n",
    "display_cols = ['latitude', 'longitude', 'assigned_method', 'knn_distance_km']\n",
    "if 'class' in df_merged.columns:\n",
    "    display_cols.append('class')\n",
    "print(df_merged[display_cols].head())\n",
    "\n",
    "print(f\"\\n‚ú® CARACT√âRISTIQUES :\")\n",
    "print(f\"   ‚úÖ 100% des points FIRE conserv√©s\")\n",
    "print(f\"   ‚úÖ Match direct pour points proches (<{DISTANCE_THRESHOLD}¬∞)\")\n",
    "print(f\"   ‚úÖ Interpolation IDW pour points √©loign√©s\")\n",
    "print(f\"   ‚úÖ Cat√©gorielles : K-NN (plus proche voisin)\")\n",
    "print(f\"   ‚úÖ Num√©riques : IDW (interpolation pond√©r√©e)\")\n",
    "print(f\"   ‚úÖ Tra√ßabilit√© via m√©tadonn√©es\")\n",
    "if 'class' in df_merged.columns:\n",
    "    print(f\"   ‚úÖ Colonne 'class' en derni√®re position\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöÄ DATASET COMPLET PR√äT POUR LA MOD√âLISATION !\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
